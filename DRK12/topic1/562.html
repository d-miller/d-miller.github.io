<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Google Tag Manager -->
    <script>
        (function (w, d, s, l, i) {
            w[l] = w[l] || []; w[l].push({ 'gtm.start': new Date().getTime(), event: 'gtm.js' });
            var f = d.getElementsByTagName(s)[0], j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true;
            j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl + '&gtm_auth=fCzlsmYQFAUQ_UXneGJNag&gtm_preview=env-2&gtm_cookies_win=x';
            f.parentNode.insertBefore(j, f);
        })(window, document, 'script', 'dataLayer', 'GTM-TZGWTGG');
    </script>
    <!-- End Google Tag Manager -->

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge" /><script type="text/javascript">window.NREUM||(NREUM={});NREUM.info = {"beacon":"bam.nr-data.net","errorBeacon":"bam.nr-data.net","licenseKey":"598a124f17","applicationID":"3007887","transactionName":"MQcDMkECCkNSW0YMWghNLDBwTCVCR1FRCVAlDQ8SQQwIXFZKHSNACg41A0sXJkl3d3s=","queueTime":0,"applicationTime":598,"agent":"","atts":""}</script><script type="text/javascript">(window.NREUM||(NREUM={})).loader_config={xpid:"VgUHUl5WGwAAVFZaDwY="};window.NREUM||(NREUM={}),__nr_require=function(t,n,e){function r(e){if(!n[e]){var o=n[e]={exports:{}};t[e][0].call(o.exports,function(n){var o=t[e][1][n];return r(o||n)},o,o.exports)}return n[e].exports}if("function"==typeof __nr_require)return __nr_require;for(var o=0;o<e.length;o++)r(e[o]);return r}({1:[function(t,n,e){function r(t){try{s.console&&console.log(t)}catch(n){}}var o,i=t("ee"),a=t(18),s={};try{o=localStorage.getItem("__nr_flags").split(","),console&&"function"==typeof console.log&&(s.console=!0,o.indexOf("dev")!==-1&&(s.dev=!0),o.indexOf("nr_dev")!==-1&&(s.nrDev=!0))}catch(c){}s.nrDev&&i.on("internal-error",function(t){r(t.stack)}),s.dev&&i.on("fn-err",function(t,n,e){r(e.stack)}),s.dev&&(r("NR AGENT IN DEVELOPMENT MODE"),r("flags: "+a(s,function(t,n){return t}).join(", ")))},{}],2:[function(t,n,e){function r(t,n,e,r,s){try{p?p-=1:o(s||new UncaughtException(t,n,e),!0)}catch(f){try{i("ierr",[f,c.now(),!0])}catch(d){}}return"function"==typeof u&&u.apply(this,a(arguments))}function UncaughtException(t,n,e){this.message=t||"Uncaught error with no additional information",this.sourceURL=n,this.line=e}function o(t,n){var e=n?null:c.now();i("err",[t,e])}var i=t("handle"),a=t(19),s=t("ee"),c=t("loader"),f=t("gos"),u=window.onerror,d=!1,l="nr@seenError",p=0;c.features.err=!0,t(1),window.onerror=r;try{throw new Error}catch(h){"stack"in h&&(t(8),t(7),"addEventListener"in window&&t(5),c.xhrWrappable&&t(9),d=!0)}s.on("fn-start",function(t,n,e){d&&(p+=1)}),s.on("fn-err",function(t,n,e){d&&!e[l]&&(f(e,l,function(){return!0}),this.thrown=!0,o(e))}),s.on("fn-end",function(){d&&!this.thrown&&p>0&&(p-=1)}),s.on("internal-error",function(t){i("ierr",[t,c.now(),!0])})},{}],3:[function(t,n,e){t("loader").features.ins=!0},{}],4:[function(t,n,e){function r(t){}if(window.performance&&window.performance.timing&&window.performance.getEntriesByType){var o=t("ee"),i=t("handle"),a=t(8),s=t(7),c="learResourceTimings",f="addEventListener",u="resourcetimingbufferfull",d="bstResource",l="resource",p="-start",h="-end",m="fn"+p,w="fn"+h,v="bstTimer",y="pushState",g=t("loader");g.features.stn=!0,t(6);var x=NREUM.o.EV;o.on(m,function(t,n){var e=t[0];e instanceof x&&(this.bstStart=g.now())}),o.on(w,function(t,n){var e=t[0];e instanceof x&&i("bst",[e,n,this.bstStart,g.now()])}),a.on(m,function(t,n,e){this.bstStart=g.now(),this.bstType=e}),a.on(w,function(t,n){i(v,[n,this.bstStart,g.now(),this.bstType])}),s.on(m,function(){this.bstStart=g.now()}),s.on(w,function(t,n){i(v,[n,this.bstStart,g.now(),"requestAnimationFrame"])}),o.on(y+p,function(t){this.time=g.now(),this.startPath=location.pathname+location.hash}),o.on(y+h,function(t){i("bstHist",[location.pathname+location.hash,this.startPath,this.time])}),f in window.performance&&(window.performance["c"+c]?window.performance[f](u,function(t){i(d,[window.performance.getEntriesByType(l)]),window.performance["c"+c]()},!1):window.performance[f]("webkit"+u,function(t){i(d,[window.performance.getEntriesByType(l)]),window.performance["webkitC"+c]()},!1)),document[f]("scroll",r,{passive:!0}),document[f]("keypress",r,!1),document[f]("click",r,!1)}},{}],5:[function(t,n,e){function r(t){for(var n=t;n&&!n.hasOwnProperty(u);)n=Object.getPrototypeOf(n);n&&o(n)}function o(t){s.inPlace(t,[u,d],"-",i)}function i(t,n){return t[1]}var a=t("ee").get("events"),s=t(21)(a,!0),c=t("gos"),f=XMLHttpRequest,u="addEventListener",d="removeEventListener";n.exports=a,"getPrototypeOf"in Object?(r(document),r(window),r(f.prototype)):f.prototype.hasOwnProperty(u)&&(o(window),o(f.prototype)),a.on(u+"-start",function(t,n){var e=t[1],r=c(e,"nr@wrapped",function(){function t(){if("function"==typeof e.handleEvent)return e.handleEvent.apply(e,arguments)}var n={object:t,"function":e}[typeof e];return n?s(n,"fn-",null,n.name||"anonymous"):e});this.wrapped=t[1]=r}),a.on(d+"-start",function(t){t[1]=this.wrapped||t[1]})},{}],6:[function(t,n,e){var r=t("ee").get("history"),o=t(21)(r);n.exports=r,o.inPlace(window.history,["pushState","replaceState"],"-")},{}],7:[function(t,n,e){var r=t("ee").get("raf"),o=t(21)(r),i="equestAnimationFrame";n.exports=r,o.inPlace(window,["r"+i,"mozR"+i,"webkitR"+i,"msR"+i],"raf-"),r.on("raf-start",function(t){t[0]=o(t[0],"fn-")})},{}],8:[function(t,n,e){function r(t,n,e){t[0]=a(t[0],"fn-",null,e)}function o(t,n,e){this.method=e,this.timerDuration=isNaN(t[1])?0:+t[1],t[0]=a(t[0],"fn-",this,e)}var i=t("ee").get("timer"),a=t(21)(i),s="setTimeout",c="setInterval",f="clearTimeout",u="-start",d="-";n.exports=i,a.inPlace(window,[s,"setImmediate"],s+d),a.inPlace(window,[c],c+d),a.inPlace(window,[f,"clearImmediate"],f+d),i.on(c+u,r),i.on(s+u,o)},{}],9:[function(t,n,e){function r(t,n){d.inPlace(n,["onreadystatechange"],"fn-",s)}function o(){var t=this,n=u.context(t);t.readyState>3&&!n.resolved&&(n.resolved=!0,u.emit("xhr-resolved",[],t)),d.inPlace(t,y,"fn-",s)}function i(t){g.push(t),h&&(b?b.then(a):w?w(a):(E=-E,R.data=E))}function a(){for(var t=0;t<g.length;t++)r([],g[t]);g.length&&(g=[])}function s(t,n){return n}function c(t,n){for(var e in t)n[e]=t[e];return n}t(5);var f=t("ee"),u=f.get("xhr"),d=t(21)(u),l=NREUM.o,p=l.XHR,h=l.MO,m=l.PR,w=l.SI,v="readystatechange",y=["onload","onerror","onabort","onloadstart","onloadend","onprogress","ontimeout"],g=[];n.exports=u;var x=window.XMLHttpRequest=function(t){var n=new p(t);try{u.emit("new-xhr",[n],n),n.addEventListener(v,o,!1)}catch(e){try{u.emit("internal-error",[e])}catch(r){}}return n};if(c(p,x),x.prototype=p.prototype,d.inPlace(x.prototype,["open","send"],"-xhr-",s),u.on("send-xhr-start",function(t,n){r(t,n),i(n)}),u.on("open-xhr-start",r),h){var b=m&&m.resolve();if(!w&&!m){var E=1,R=document.createTextNode(E);new h(a).observe(R,{characterData:!0})}}else f.on("fn-end",function(t){t[0]&&t[0].type===v||a()})},{}],10:[function(t,n,e){function r(){var t=window.NREUM,n=t.info.accountID||null,e=t.info.agentID||null,r=t.info.trustKey||null,i="btoa"in window&&"function"==typeof window.btoa;if(!n||!e||!i)return null;var a={v:[0,1],d:{ty:"Browser",ac:n,ap:e,id:o.generateCatId(),tr:o.generateCatId(),ti:Date.now()}};return r&&n!==r&&(a.d.tk=r),btoa(JSON.stringify(a))}var o=t(16);n.exports={generateTraceHeader:r}},{}],11:[function(t,n,e){function r(t){var n=this.params,e=this.metrics;if(!this.ended){this.ended=!0;for(var r=0;r<p;r++)t.removeEventListener(l[r],this.listener,!1);n.aborted||(e.duration=s.now()-this.startTime,this.loadCaptureCalled||4!==t.readyState?null==n.status&&(n.status=0):a(this,t),e.cbTime=this.cbTime,d.emit("xhr-done",[t],t),c("xhr",[n,e,this.startTime]))}}function o(t,n){var e=t.responseType;if("json"===e&&null!==n)return n;var r="arraybuffer"===e||"blob"===e||"json"===e?t.response:t.responseText;return w(r)}function i(t,n){var e=f(n),r=t.params;r.host=e.hostname+":"+e.port,r.pathname=e.pathname,t.sameOrigin=e.sameOrigin}function a(t,n){t.params.status=n.status;var e=o(n,t.lastSize);if(e&&(t.metrics.rxSize=e),t.sameOrigin){var r=n.getResponseHeader("X-NewRelic-App-Data");r&&(t.params.cat=r.split(", ").pop())}t.loadCaptureCalled=!0}var s=t("loader");if(s.xhrWrappable){var c=t("handle"),f=t(12),u=t(10).generateTraceHeader,d=t("ee"),l=["load","error","abort","timeout"],p=l.length,h=t("id"),m=t(15),w=t(14),v=window.XMLHttpRequest;s.features.xhr=!0,t(9),d.on("new-xhr",function(t){var n=this;n.totalCbs=0,n.called=0,n.cbTime=0,n.end=r,n.ended=!1,n.xhrGuids={},n.lastSize=null,n.loadCaptureCalled=!1,t.addEventListener("load",function(e){a(n,t)},!1),m&&(m>34||m<10)||window.opera||t.addEventListener("progress",function(t){n.lastSize=t.loaded},!1)}),d.on("open-xhr-start",function(t){this.params={method:t[0]},i(this,t[1]),this.metrics={}}),d.on("open-xhr-end",function(t,n){"loader_config"in NREUM&&"xpid"in NREUM.loader_config&&this.sameOrigin&&n.setRequestHeader("X-NewRelic-ID",NREUM.loader_config.xpid);var e=!1;if("init"in NREUM&&"distributed_tracing"in NREUM.init&&(e=!!NREUM.init.distributed_tracing.enabled),e&&this.sameOrigin){var r=u();r&&n.setRequestHeader("newrelic",r)}}),d.on("send-xhr-start",function(t,n){var e=this.metrics,r=t[0],o=this;if(e&&r){var i=w(r);i&&(e.txSize=i)}this.startTime=s.now(),this.listener=function(t){try{"abort"!==t.type||o.loadCaptureCalled||(o.params.aborted=!0),("load"!==t.type||o.called===o.totalCbs&&(o.onloadCalled||"function"!=typeof n.onload))&&o.end(n)}catch(e){try{d.emit("internal-error",[e])}catch(r){}}};for(var a=0;a<p;a++)n.addEventListener(l[a],this.listener,!1)}),d.on("xhr-cb-time",function(t,n,e){this.cbTime+=t,n?this.onloadCalled=!0:this.called+=1,this.called!==this.totalCbs||!this.onloadCalled&&"function"==typeof e.onload||this.end(e)}),d.on("xhr-load-added",function(t,n){var e=""+h(t)+!!n;this.xhrGuids&&!this.xhrGuids[e]&&(this.xhrGuids[e]=!0,this.totalCbs+=1)}),d.on("xhr-load-removed",function(t,n){var e=""+h(t)+!!n;this.xhrGuids&&this.xhrGuids[e]&&(delete this.xhrGuids[e],this.totalCbs-=1)}),d.on("addEventListener-end",function(t,n){n instanceof v&&"load"===t[0]&&d.emit("xhr-load-added",[t[1],t[2]],n)}),d.on("removeEventListener-end",function(t,n){n instanceof v&&"load"===t[0]&&d.emit("xhr-load-removed",[t[1],t[2]],n)}),d.on("fn-start",function(t,n,e){n instanceof v&&("onload"===e&&(this.onload=!0),("load"===(t[0]&&t[0].type)||this.onload)&&(this.xhrCbStart=s.now()))}),d.on("fn-end",function(t,n){this.xhrCbStart&&d.emit("xhr-cb-time",[s.now()-this.xhrCbStart,this.onload,n],n)})}},{}],12:[function(t,n,e){n.exports=function(t){var n=document.createElement("a"),e=window.location,r={};n.href=t,r.port=n.port;var o=n.href.split("://");!r.port&&o[1]&&(r.port=o[1].split("/")[0].split("@").pop().split(":")[1]),r.port&&"0"!==r.port||(r.port="https"===o[0]?"443":"80"),r.hostname=n.hostname||e.hostname,r.pathname=n.pathname,r.protocol=o[0],"/"!==r.pathname.charAt(0)&&(r.pathname="/"+r.pathname);var i=!n.protocol||":"===n.protocol||n.protocol===e.protocol,a=n.hostname===document.domain&&n.port===e.port;return r.sameOrigin=i&&(!n.hostname||a),r}},{}],13:[function(t,n,e){function r(){}function o(t,n,e){return function(){return i(t,[f.now()].concat(s(arguments)),n?null:this,e),n?void 0:this}}var i=t("handle"),a=t(18),s=t(19),c=t("ee").get("tracer"),f=t("loader"),u=NREUM;"undefined"==typeof window.newrelic&&(newrelic=u);var d=["setPageViewName","setCustomAttribute","setErrorHandler","finished","addToTrace","inlineHit","addRelease"],l="api-",p=l+"ixn-";a(d,function(t,n){u[n]=o(l+n,!0,"api")}),u.addPageAction=o(l+"addPageAction",!0),u.setCurrentRouteName=o(l+"routeName",!0),n.exports=newrelic,u.interaction=function(){return(new r).get()};var h=r.prototype={createTracer:function(t,n){var e={},r=this,o="function"==typeof n;return i(p+"tracer",[f.now(),t,e],r),function(){if(c.emit((o?"":"no-")+"fn-start",[f.now(),r,o],e),o)try{return n.apply(this,arguments)}catch(t){throw c.emit("fn-err",[arguments,this,t],e),t}finally{c.emit("fn-end",[f.now()],e)}}}};a("actionText,setName,setAttribute,save,ignore,onEnd,getContext,end,get".split(","),function(t,n){h[n]=o(p+n)}),newrelic.noticeError=function(t,n){"string"==typeof t&&(t=new Error(t)),i("err",[t,f.now(),!1,n])}},{}],14:[function(t,n,e){n.exports=function(t){if("string"==typeof t&&t.length)return t.length;if("object"==typeof t){if("undefined"!=typeof ArrayBuffer&&t instanceof ArrayBuffer&&t.byteLength)return t.byteLength;if("undefined"!=typeof Blob&&t instanceof Blob&&t.size)return t.size;if(!("undefined"!=typeof FormData&&t instanceof FormData))try{return JSON.stringify(t).length}catch(n){return}}}},{}],15:[function(t,n,e){var r=0,o=navigator.userAgent.match(/Firefox[\/\s](\d+\.\d+)/);o&&(r=+o[1]),n.exports=r},{}],16:[function(t,n,e){function r(){function t(){return n?15&n[e++]:16*Math.random()|0}var n=null,e=0,r=window.crypto||window.msCrypto;r&&r.getRandomValues&&(n=r.getRandomValues(new Uint8Array(31)));for(var o,i="xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx",a="",s=0;s<i.length;s++)o=i[s],"x"===o?a+=t().toString(16):"y"===o?(o=3&t()|8,a+=o.toString(16)):a+=o;return a}function o(){function t(){return n?15&n[e++]:16*Math.random()|0}var n=null,e=0,r=window.crypto||window.msCrypto;r&&r.getRandomValues&&Uint8Array&&(n=r.getRandomValues(new Uint8Array(31)));for(var o=[],i=0;i<16;i++)o.push(t().toString(16));return o.join("")}n.exports={generateUuid:r,generateCatId:o}},{}],17:[function(t,n,e){function r(t,n){if(!o)return!1;if(t!==o)return!1;if(!n)return!0;if(!i)return!1;for(var e=i.split("."),r=n.split("."),a=0;a<r.length;a++)if(r[a]!==e[a])return!1;return!0}var o=null,i=null,a=/Version\/(\S+)\s+Safari/;if(navigator.userAgent){var s=navigator.userAgent,c=s.match(a);c&&s.indexOf("Chrome")===-1&&s.indexOf("Chromium")===-1&&(o="Safari",i=c[1])}n.exports={agent:o,version:i,match:r}},{}],18:[function(t,n,e){function r(t,n){var e=[],r="",i=0;for(r in t)o.call(t,r)&&(e[i]=n(r,t[r]),i+=1);return e}var o=Object.prototype.hasOwnProperty;n.exports=r},{}],19:[function(t,n,e){function r(t,n,e){n||(n=0),"undefined"==typeof e&&(e=t?t.length:0);for(var r=-1,o=e-n||0,i=Array(o<0?0:o);++r<o;)i[r]=t[n+r];return i}n.exports=r},{}],20:[function(t,n,e){n.exports={exists:"undefined"!=typeof window.performance&&window.performance.timing&&"undefined"!=typeof window.performance.timing.navigationStart}},{}],21:[function(t,n,e){function r(t){return!(t&&t instanceof Function&&t.apply&&!t[a])}var o=t("ee"),i=t(19),a="nr@original",s=Object.prototype.hasOwnProperty,c=!1;n.exports=function(t,n){function e(t,n,e,o){function nrWrapper(){var r,a,s,c;try{a=this,r=i(arguments),s="function"==typeof e?e(r,a):e||{}}catch(f){l([f,"",[r,a,o],s])}u(n+"start",[r,a,o],s);try{return c=t.apply(a,r)}catch(d){throw u(n+"err",[r,a,d],s),d}finally{u(n+"end",[r,a,c],s)}}return r(t)?t:(n||(n=""),nrWrapper[a]=t,d(t,nrWrapper),nrWrapper)}function f(t,n,o,i){o||(o="");var a,s,c,f="-"===o.charAt(0);for(c=0;c<n.length;c++)s=n[c],a=t[s],r(a)||(t[s]=e(a,f?s+o:o,i,s))}function u(e,r,o){if(!c||n){var i=c;c=!0;try{t.emit(e,r,o,n)}catch(a){l([a,e,r,o])}c=i}}function d(t,n){if(Object.defineProperty&&Object.keys)try{var e=Object.keys(t);return e.forEach(function(e){Object.defineProperty(n,e,{get:function(){return t[e]},set:function(n){return t[e]=n,n}})}),n}catch(r){l([r])}for(var o in t)s.call(t,o)&&(n[o]=t[o]);return n}function l(n){try{t.emit("internal-error",n)}catch(e){}}return t||(t=o),e.inPlace=f,e.flag=a,e}},{}],ee:[function(t,n,e){function r(){}function o(t){function n(t){return t&&t instanceof r?t:t?c(t,s,i):i()}function e(e,r,o,i){if(!l.aborted||i){t&&t(e,r,o);for(var a=n(o),s=m(e),c=s.length,f=0;f<c;f++)s[f].apply(a,r);var d=u[g[e]];return d&&d.push([x,e,r,a]),a}}function p(t,n){y[t]=m(t).concat(n)}function h(t,n){var e=y[t];if(e)for(var r=0;r<e.length;r++)e[r]===n&&e.splice(r,1)}function m(t){return y[t]||[]}function w(t){return d[t]=d[t]||o(e)}function v(t,n){f(t,function(t,e){n=n||"feature",g[e]=n,n in u||(u[n]=[])})}var y={},g={},x={on:p,addEventListener:p,removeEventListener:h,emit:e,get:w,listeners:m,context:n,buffer:v,abort:a,aborted:!1};return x}function i(){return new r}function a(){(u.api||u.feature)&&(l.aborted=!0,u=l.backlog={})}var s="nr@context",c=t("gos"),f=t(18),u={},d={},l=n.exports=o();l.backlog=u},{}],gos:[function(t,n,e){function r(t,n,e){if(o.call(t,n))return t[n];var r=e();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(t,n,{value:r,writable:!0,enumerable:!1}),r}catch(i){}return t[n]=r,r}var o=Object.prototype.hasOwnProperty;n.exports=r},{}],handle:[function(t,n,e){function r(t,n,e,r){o.buffer([t],r),o.emit(t,n,e)}var o=t("ee").get("handle");n.exports=r,r.ee=o},{}],id:[function(t,n,e){function r(t){var n=typeof t;return!t||"object"!==n&&"function"!==n?-1:t===window?0:a(t,i,function(){return o++})}var o=1,i="nr@id",a=t("gos");n.exports=r},{}],loader:[function(t,n,e){function r(){if(!E++){var t=b.info=NREUM.info,n=p.getElementsByTagName("script")[0];if(setTimeout(u.abort,3e4),!(t&&t.licenseKey&&t.applicationID&&n))return u.abort();f(g,function(n,e){t[n]||(t[n]=e)}),c("mark",["onload",a()+b.offset],null,"api");var e=p.createElement("script");e.src="https://"+t.agent,n.parentNode.insertBefore(e,n)}}function o(){"complete"===p.readyState&&i()}function i(){c("mark",["domContent",a()+b.offset],null,"api")}function a(){return R.exists&&performance.now?Math.round(performance.now()):(s=Math.max((new Date).getTime(),s))-b.offset}var s=(new Date).getTime(),c=t("handle"),f=t(18),u=t("ee"),d=t(17),l=window,p=l.document,h="addEventListener",m="attachEvent",w=l.XMLHttpRequest,v=w&&w.prototype;NREUM.o={ST:setTimeout,SI:l.setImmediate,CT:clearTimeout,XHR:w,REQ:l.Request,EV:l.Event,PR:l.Promise,MO:l.MutationObserver};var y=""+location,g={beacon:"bam.nr-data.net",errorBeacon:"bam.nr-data.net",agent:"js-agent.newrelic.com/nr-1123.min.js"},x=w&&v&&v[h]&&!/CriOS/.test(navigator.userAgent),b=n.exports={offset:s,now:a,origin:y,features:{},xhrWrappable:x,userAgent:d};t(13),p[h]?(p[h]("DOMContentLoaded",i,!1),l[h]("load",r,!1)):(p[m]("onreadystatechange",o),l[m]("onload",r)),c("mark",["firstbyte",s],null,"api");var E=0,R=t(20)},{}]},{},["loader",2,11,4,3]);</script>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <title>Frontiers | Mobile Mixed-Reality Interfaces That Enhance Human–Robot Interaction in Shared Spaces | Robotics and AI</title>

    <link rel="shortcut icon" href="https://3718aeafc638f96f5bd6-d4a9ca15fc46ba40e71f94dec0aad28c.ssl.cf1.rackcdn.com/favicon_16x16.ico" type="image/x-icon" />

    <meta property="og:type" content="article" />
    <meta property="frontiers:type" content="Article" />
    <meta property="og:site_name" name="site_name" content="Frontiers" />
    <meta property="og:title" name="Title" content="Mobile Mixed-Reality Interfaces That Enhance Human–Robot Interaction in Shared Spaces" />
    <meta property="og:description" name="Description" content="Although user interfaces with gesture-based input and augmented graphics have promoted intuitive human-robot interactions (HRI), they are often implemented in remote applications on research-grade platforms requiring significant training and limiting  operator mobility. This paper proposes a mobile mixed-reality interface approach to enhance HRI in shared spaces. As a user points a mobile device at the robot's workspace, a mixed-reality environment is rendered providing a common frame of reference for the user and robot to effectively communicate spatial information for performing object manipulation tasks, improving the user's situational awareness while interacting with augmented graphics to intuitively command the robot. An evaluation with participants is conducted to examine task performance and user experience associated with the proposed interface strategy in comparison to conventional approaches that utilize egocentric or exocentric views from cameras mounted on the robot or in the environment, respectively. Results indicate that, despite the suitability of the conventional approaches in remote applications, the proposed interface approach provides comparable task performance and user experiences in shared spaces without the need to install operator stations or vision systems on or around the robot. Moreover, the proposed interface approach provides users the flexibility to direct robots from their own visual perspective (at the expense of some physical workload) an..." />
    <meta property="og:url" name="url" content="https://www.frontiersin.org/article/10.3389/frobt.2017.00020/full" />
        <meta property="og:image" content="https://www.frontiersin.org/files/MyHome%20Article%20Library/238204/238204_Thumb_400.jpg" />
            <meta name="citation_volume" content="4" />
        <meta name="citation_journal_title" content="Frontiers in Robotics and AI" />
        <meta name="citation_publisher" content="Frontiers" />
        <meta name="citation_journal_abbrev" content="Front. Robot. AI" />
        <meta name="citation_issn" content="2296-9144" />
        <meta name="citation_doi" content="10.3389/frobt.2017.00020" />
        <meta name="citation_pages" content="20" />
        <meta name="citation_language" content="English" />
        <meta name="citation_title" content="Mobile Mixed-Reality Interfaces That Enhance Human–Robot Interaction in Shared Spaces" />
        <meta name="citation_keywords" content="Interaction; interface; Robotics; Manipulation; mixed-reality; tablet; Vision; workspace" />
        <meta name="citation_abstract" content="Although user interfaces with gesture-based input and augmented graphics have promoted intuitive human-robot interactions (HRI), they are often implemented in remote applications on research-grade platforms requiring significant training and limiting operator mobility. This paper proposes a mobile mixed-reality interface approach to enhance HRI in shared spaces. As a user points a mobile device at the robot&#39;s workspace, a mixed-reality environment is rendered providing a common frame of reference for the user and robot to effectively communicate spatial information for performing object manipulation tasks, improving the user&#39;s situational awareness while interacting with augmented graphics to intuitively command the robot. An evaluation with participants is conducted to examine task performance and user experience associated with the proposed interface strategy in comparison to conventional approaches that utilize egocentric or exocentric views from cameras mounted on the robot or in the environment, respectively. Results indicate that, despite the suitability of the conventional approaches in remote applications, the proposed interface approach provides comparable task performance and user experiences in shared spaces without the need to install operator stations or vision systems on or around the robot. Moreover, the proposed interface approach provides users the flexibility to direct robots from their own visual perspective (at the expense of some physical workload) and leverages the sensing capabilities of the tablet to expand the robot&#39;s perceptual range." />
        <meta name="description" content="Although user interfaces with gesture-based input and augmented graphics have promoted intuitive human-robot interactions (HRI), they are often implemented in remote applications on research-grade platforms requiring significant training and limiting operator mobility. This paper proposes a mobile mixed-reality interface approach to enhance HRI in shared spaces. As a user points a mobile device at the robot&#39;s workspace, a mixed-reality environment is rendered providing a common frame of reference for the user and robot to effectively communicate spatial information for performing object manipulation tasks, improving the user&#39;s situational awareness while interacting with augmented graphics to intuitively command the robot. An evaluation with participants is conducted to examine task performance and user experience associated with the proposed interface strategy in comparison to conventional approaches that utilize egocentric or exocentric views from cameras mounted on the robot or in the environment, respectively. Results indicate that, despite the suitability of the conventional approaches in remote applications, the proposed interface approach provides comparable task performance and user experiences in shared spaces without the need to install operator stations or vision systems on or around the robot. Moreover, the proposed interface approach provides users the flexibility to direct robots from their own visual perspective (at the expense of some physical workload) and leverages the sensing capabilities of the tablet to expand the robot&#39;s perceptual range." />
        <meta name="citation_online_date" content="2017/05/10" />
        <meta name="citation_date" content="2017" />
        <meta name="citation_publication_date" content="2017/06/09" />
        <meta name="citation_pdf_url" content="https://www.frontiersin.org/articles/10.3389/frobt.2017.00020/pdf" />
        <meta name="citation_author" content="Frank, Jared A." />
        <meta name="citation_author_institution" content="Mechatronics, Controls, and Robotics Laboratory, Mechanical and Aerospace Engineering Department, NYU Tandon School of Engineering, USA" />
        <meta name="citation_author_email" content="" />
        <meta name="citation_author" content="Moorhead, Matthew" />
        <meta name="citation_author_institution" content="Mechatronics, Controls, and Robotics Laboratory, Mechanical and Aerospace Engineering Department, NYU Tandon School of Engineering, USA" />
        <meta name="citation_author_email" content="" />
        <meta name="citation_author" content="Kapila, Vikram" />
        <meta name="citation_author_institution" content="Mechatronics, Controls, and Robotics Laboratory, Mechanical and Aerospace Engineering Department, NYU Tandon School of Engineering, USA" />
        <meta name="citation_author_email" content="vkapila@nyu.edu" />
    <meta name="Keywords" content="Interaction, interface, Robotics, Manipulation, mixed-reality, tablet, Vision, workspace" />

        <meta name="dc.identifier" content="doi:10.3389/frobt.2017.00020"> <!--CrossMark widget-->

    <script type="text/javascript">

        var CurrentIBarMenu = 'bysubjects';
        var CurrentPageCode = 'ARTICLE_PAGE_NEW';

        var FRConfiguration = (function() {
            return {
                Environment: 'Live',
                SANVirtualPath: 'http://www.frontiersin.org/files/',
                SharepointWebsiteUrl: 'https://www.frontiersin.org',
                CommunityWebsiteUrl: 'http://community.frontiersin.org',
                FrontiersJournalUIUrl: 'https://www.frontiersin.org',
                FrontiersJournalAPIUrl: 'https://api-journal.frontiersin.org',
                FrontiersReviewUIUrl: '',
                FrontiersReviewAPIUrl: '',
                FrontiersNetworkingAPIUrl: 'https://api-network.frontiersin.org',
                FrontiersCookie: 'frontiersN',
                FrontiersCookieRememberMe: 'frontiersNt',
                FrontiersLoginUrl: 'https://www.frontiersin.org/Login.aspx',
                FrontiersRegistrationUrl: 'http://www.frontiersin.org/Registration/Register.aspx',
                IsJournalCommentApiEnabled : 'True'
            };
        })();

        var FRLanguage = (function() {
            var languageSet = {"ART_FRONTIERS":"Frontiers ","Article_AcceptedDate":"Accepted: ","Article_AnalyticsToolTip":"The total view count is updated once a day, so not to worry if you don\u0027t see immediate results.","Article_AnalyticsTotalViews":"total views","Article_AnalyticsViewImpact":"View Article Impact","Article_ArchiveLinkText":"Articles","Article_BibTex":"BibTex","Article_Citation":"Citation: ","Article_Commentary":"COMMENTARY","Article_Copyright":"Copyright: ","Article_CopyrightText":"This is an open-access article distributed under the terms of the \u003ca href=\"http://creativecommons.org/licenses/by/4.0/\"\u003eCreative Commons Attribution License (CC BY)\u003c/a\u003e. The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.","Article_Correspondence":"* Correspondence: ","Article_DownloadArticle":"Download Article","Article_DownloadPDF":"Download PDF","Article_DownloadProvisionalArticle":"Download Provisional Article","Article_DownloadProvisionalPDF":"Download Provisional PDF","Article_EditedBy":"Edited by: ","Article_EndNote":"EndNote","Article_EPUB":"EPUB","Article_ExportCitation":"Export Citation","Article_JATS":"JATS","Article_Keywords":"Keywords: ","Article_NLM":"XML (NLM)","Article_OriginalArticle":"ORIGINAL ARTICLE","Article_PaperPendingPublishedDate":"Paper pending published: ","Article_PDF":"PDF","Article_ProvisionalPDF":"Provisional PDF","Article_PublishedDate":"Published online: ","Article_ReadFullText":"Read Full Text","Article_ReceivedDate":"Received: ","Article_ReferenceManager":"Reference Manager","Article_ReviewedBy":"Reviewed by: ","Article_RTInfoText":"This article is part of the Research Topic","Article_ShareOn":"SHARE ON","Article_SimpleTEXTfile":"Simple TEXT file","Article_SupplementalData":"SUPPLEMENTAL DATA","Article_TableOfContent":"TABLE OF CONTENTS","Article_ViewEnhancedPDF":"ReadCube","Article_XML":"XML","BrowserWarningText":"\u003ch2\u003eWarning!\u003c/h2\u003e\u003cp\u003eYou are using an \u003cstrong\u003eoutdated\u003c/strong\u003e browser. This page doesn\u0027t support Internet Explorer 6, 7 and 8.\u003cbr /\u003ePlease \u003ca class=\"blue\" href=\"http://browsehappy.com/\"\u003eupgrade your browser\u003c/a\u003e or \u003ca class=\"blue\" href=\"http://www.google.com/chromeframe/?redirect=true\"\u003eactivate Google Chrome Frame\u003c/a\u003e to improve your experience.\u003c/p\u003e","COMMENT_HEADERTEXT":"Comment text too long","COMMENT_WARNINGTEXT":"Comments must be less than 4,000 characters. You have entered ","Impact_BackToArticle":"Back to article","People_Also_LookedAt":"People also looked at"};

            return {
                value: function(key) {
                    if (languageSet[key]) {
                        return languageSet[key];
                    } else {
                        throw new Error('Unable to get the value status from the language set'); // Use Error, not FRError
                    }
                }
            };

        })();

        var FRJournalDetails = (function() {
            return {
                JournalType: 'section',
                JournalId: '657',
                SectionId: '1186'
            };
        })();

        var FRArticleRecaptchaSettings = (function() {
            return {
                RecaptchaSiteKey: '6LdG3i0UAAAAAOC4qUh35ubHgJotEHp_STXHgr_v'
            };
        })();

    </script>

    <script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-535f7e222fc3ca72"></script>


    <script language="javascript">
        var addthis_config = addthis_config || {};
        addthis_config.data_track_addressbar = false;
        addthis_config.data_track_clickback = false;
    </script>

    
    
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>

      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>

    <![endif]-->

    <script src="https://maps.googleapis.com/maps/api/js?sensor=false"></script>

    <script src="/areas/research-topics/scripts/frontiers/common/frontiers.markerclusterer.js"></script>



    <link href="https://209cd62b0febf2a55d40-715eb384bc6027b876e93ad33d5c5ec3.ssl.cf3.rackcdn.com/font-awesome-4.3.0/css/font-awesome.css" rel="stylesheet"/>

    <link href="https://9d0dd7a648345f19af83-877d2ecaf11b88d5e17327c758e17ef6.ssl.cf2.rackcdn.com/museo-sans-1.0.1/css/museo-sans.css" rel="stylesheet"/>

    <link href="https://9d0dd7a648345f19af83-877d2ecaf11b88d5e17327c758e17ef6.ssl.cf2.rackcdn.com/museo-slab-1.0.1/css/museo-slab.css" rel="stylesheet"/>

    <link href="https://209cd62b0febf2a55d40-715eb384bc6027b876e93ad33d5c5ec3.ssl.cf3.rackcdn.com/open-sans-1.0.0/css/open-sans.css" rel="stylesheet"/>

    <link href="/areas/articles/css/app?v=CaLtx65_hB9Riy9_q6VeIw6cLd_LOO_8A1dCd8RogGk1" rel="stylesheet"/>

    
    <style type="text/css"> .journal-robotics-and-ai {
 background: #fff url('https://3718aeafc638f96f5bd6-d4a9ca15fc46ba40e71f94dec0aad28c.ssl.cf1.rackcdn.com/journal-robotics-and-ai.png')  no-repeat 105% 0px;
	background-size: 41%;
}
/* Displays/Screens (e.g. 19" WS @ 1440x900) --------------- */ @media only screen and (max-width: 1649px) {
.journal-robotics-and-ai {
  background: #fff url('https://3718aeafc638f96f5bd6-d4a9ca15fc46ba40e71f94dec0aad28c.ssl.cf1.rackcdn.com/journal-robotics-and-ai.png') no-repeat 120% 30px;
 background-size: 53%;
 }}
/* Displays/Screens (e.g. MacBook @ 1280x800) -------------- */
@media only screen and (max-width: 1409px) {
.journal-robotics-and-ai {
  background: #fff url('https://3718aeafc638f96f5bd6-d4a9ca15fc46ba40e71f94dec0aad28c.ssl.cf1.rackcdn.com/journal-robotics-and-ai.png') no-repeat 120% 17px;
 background-size: 48%;
 }}
/* Large Devices, Wide Screens --------- */  @media only screen and (max-width : 1250px) {
.journal-robotics-and-ai {
  background: #fff url('https://3718aeafc638f96f5bd6-d4a9ca15fc46ba40e71f94dec0aad28c.ssl.cf1.rackcdn.com/journal-robotics-and-ai.png') no-repeat 120% 30px;
 background-size: 48%;
 }}
/* Medium Devices, Desktops ---------- */  @media only screen and (max-width : 992px) { 
.journal-robotics-and-ai {
  background: #fff url('https://3718aeafc638f96f5bd6-d4a9ca15fc46ba40e71f94dec0aad28c.ssl.cf1.rackcdn.com/journal-robotics-and-ai.png') no-repeat 100% 30px;
 background-size: 47%;
 }}
/* Small Devices, Tablets ------------ */  @media only screen and (max-width : 768px) {
.journal-robotics-and-ai {
  background: #fff url('https://3718aeafc638f96f5bd6-d4a9ca15fc46ba40e71f94dec0aad28c.ssl.cf1.rackcdn.com/journal-robotics-and-ai.png') no-repeat 100% 50px;
 background-size: 65%;
 }}

/* Small Devices, Tablets  --------- */ @media only screen and (max-width : 640px) {
.journal-robotics-and-ai {
  background: #fff url('https://3718aeafc638f96f5bd6-d4a9ca15fc46ba40e71f94dec0aad28c.ssl.cf1.rackcdn.com/journal-robotics-and-ai.png') no-repeat 135% 50px;
 background-size: 75%;
 }}

/* Extra Small Devices, Phones-----------  */  @media only screen and (max-width : 480px) { 
.journal-robotics-and-ai {
  background: #fff url('https://3718aeafc638f96f5bd6-d4a9ca15fc46ba40e71f94dec0aad28c.ssl.cf1.rackcdn.com/journal-robotics-and-ai.png') no-repeat 130% 50px;
 background-size: 75%;
 }} </style>
    
</head>
<body class="journal-robotics-and-ai section-robot-and-machine-vision">
    <!-- Google Tag Manager (noscript) -->
    <noscript>
        <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TZGWTGG&gtm_auth=fCzlsmYQFAUQ_UXneGJNag&gtm_preview=env-2&gtm_cookies_win=x"
                height="0" width="0" style="display:none;visibility:hidden"></iframe>
    </noscript>
    <!-- End Google Tag Manager (noscript) -->


    <link href="https://api-journal.frontiersin.org/areas/header/content/styles/frontiers.header.v3.css" rel="stylesheet"></link><div class="header-compact">
    <div class="navbar navbar-fixed-top">
        <div class="row container">
            <div class="navbar-header">
                <div class="col-xs-6 header-left-container">
                    <ul class="navbar-toggle inline-list navbar-mobile" data-toggle="collapse" data-target="#">
    <li class="no-left-margin">
        <a class="brand home" href="https://www.frontiersin.org">
            <img class="header-logo">
        </a>
    </li>
    <li class="clickable no-left-margin">
        <div class="journal-dropdown-responsive">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
        </div>
    </li>
</ul>

<ul class="inline-list navbar-collapse collapse navbar-desktop">
    <li class="header-inline-element no-left-margin">
        <a class="brand home" href="https://www.frontiersin.org" onclick="FRHeader.trackOutboundLink('Header_Action', 'link', 'click_header_home', 'https://www.frontiersin.org');return false;">
            <img class="header-logo">
        </a>
    </li>
    <li class="no-left-margin">
        <a class="home" href="https://www.frontiersin.org" onclick="FRHeader.trackOutboundLink('Header_Action', 'link', 'click_header_home', 'https://www.frontiersin.org');return false;">
            Home
        </a>
    </li>
    <li class="no-left-margin">
        <a class="about" href="https://www.frontiersin.org/AboutFrontiers.aspx" onclick="FRHeader.trackOutboundLink('Header_Action', 'link', 'click_header_about', 'https://www.frontiersin.org/AboutFrontiers.aspx');return false;">
            About
        </a>
    </li>
    <li class="submit-container">
        <a class="submit" href="https://www.frontiersin.org/Submission/SubmissionInfo.aspx" onclick="FRHeader.trackOutboundLink('Header_Action', 'link', 'click_header_submit', 'https://www.frontiersin.org/Submission/SubmissionInfo.aspx');return false;">
            Submit
        </a>
    </li>
    <li class="no-padding-right journal-drop hidden">
        <a class="byjournal bysubjects" data-test-id="journal-bydropdown">
            <span>Journals</span><img class="drop-down-arrow" src="https://api-journal.frontiersin.org/Areas/Header/Content/Images/header-arrow-down.png" alt="" />
        </a>
    </li>
    <li class="journalAZ hidden">
        <a class="bysubjects" href="https://www.frontiersin.org/AboutFrontiers.aspx?stage=journalseries" data-test-id="journal-atoz">
            Journals A-Z
        </a>
    </li>
    
    <li class="research-topic-container">
        <a class="research-topic" href="https://www.frontiersin.org/research-topics" onclick="FRHeader.trackOutboundLink('Header_Action', 'link', 'click_header_rt', 'https://www.frontiersin.org/research-topics');return false;">
            Research Topics
        </a>
    </li>
    
</ul>

                </div>
                <div class="header-search">
                    <div class="search-swappable hidden-xs">
    <a class="search-icon-container hidden" data-test-id="search-icon">
        <img class="search-icon" src="https://api-journal.frontiersin.org/Areas/Header/Content/Images/header-search.png" />
    </a>
    <form class="search-bar-container hide">
        <input data-test-id="search-field" placeholder="Search for articles, people, events and more." class="header-search-field">
        <input data-test-id="search-btn" type="button" class="header-search-btn" value="">
    </form>
</div>

                </div>
                <div class="col-xs-6 pull-right unlogged-user hidden">


    <ul class="inline-list pull-right">
        <li>
            <a class="login-container" data-test-id="login-link" href="https://www.frontiersin.org/people/login" data-header-tracking='{ "category": "Header_Action", "action": "link", "label": "click_header_login" }'>
                <span>Login</span>
            </a>
        </li>
        <li>
            <a class="register-container" data-test-id="register-link" href="https://www.frontiersin.org/register" >
                <span>Register</span>
            </a>
        </li>
    </ul>

<div class="popover-wrapper popover-login" style="display: none">
    <div class="popover-menu login-popover">
        <form action="https://www.frontiersin.org/Login.aspx" name="login-form" method="POST" class="login-form">
            <div class="third-party-signin-row row-inside-column">
                <h1>Login using</h1>
                <div class="third-party-buttons">
                    <div class="third-party-button-wrapper" id="third-party-btn-for-linkedin">
                        <a data-type="3">
                            <div class="radius">
                                <div class="third-party-icon linkedin-icon-wrapper">
                                    <div class="linkedin-icon"></div>
                                </div>
                            </div>
                            <div class="third-party-name">LinkedIn</div>
                        </a>
                    </div><div class="third-party-button-wrapper" id="third-party-btn-for-twitter">
                        <a data-type="2">
                            <div class="radius">
                                <div class="third-party-icon twitter-icon-wrapper">
                                    <div class="twitter-icon"></div>
                                </div>
                            </div>
                            <div class="third-party-name">Twitter</div>
                        </a>
                    </div>
                    <div class="third-party-button-wrapper" id="third-party-btn-for-facebook">
                        <a data-type="1">
                            <div class="radius">
                                <div class="third-party-icon facebook-icon-wrapper">
                                    <div class="facebook-icon"></div>
                                </div>
                            </div>
                            <div class="third-party-name">Facebook</div>
                        </a>
                    </div>
                    <div class="info-button-wrapper" id="third-party-information-button">
                        <a class="popover-login-info-trigger">
                            <div class="info-button">i</div>
                        </a>
                        <div class="popover-wrapper popover-login-info" style="display: none">
                            <div class="popover-menu">
                                <h1 class="popover-title">You can login by using one of your existing accounts.</h1>
                                <span style="float: none; display: inline">
                                    We will be provided with an authorization token (please note: passwords are not shared with us) and will sync your accounts for you. This means that you will not need to remember your user name and password in the future and you will be able to login with the account you choose to sync, with the click of a button.
                                </span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div class="clear"></div>
            <div class="or-row" id="or-circle-divider">
                <div class="or-divider or-divider-inside-column">
                    <div class="or-circle">OR</div>
                </div>
            </div>
            <div class="loginForm">
                <div class="form-field-label">Email *</div>
                <div class="form-field">
                    <input runat="server" tabindex="5" name="txtLoginEmail" type="text">
                    <div class="form-field-error-msg">
                        <div class="error-icon-small"></div>
                        Please fill in this field
                        <div class="clear"></div>
                    </div>
                </div>
                <div class="form-field-label">Password *</div>
                <div class="form-field">
                    <input runat="server" tabindex="6" name="txtLoginPassword" type="password">
                    <div class="form-field-error-msg">
                        <div class="error-icon-small"></div>
                        Please fill in this field
                        <div class="clear"></div>
                    </div>
                </div>
                <div id="rememberMeWrapper">
                    <div class="checkbox-remember-container">
                        <input runat="server" tabindex="7" name="chkLoginRememberMe" type="checkbox">
                    </div>
                    <div class="checkbox-remember-label">Remember me</div>
                    <div class="forgot-password">
                        <a href="https://registration.frontiersin.org/people/forgot-password">Forgot Password?</a>
                    </div>
                </div>
                <div class="login-page-submit" id="formSubmitWrapper">
                    <div>
                        <input name="hdnIsPopupLogin" runat="server" value="1" type="hidden">
                        <button type="submit" tabindex="8" class="btn btn-loginlogout-blue btn-flat input-block-level">Login</button>
                    </div>
                </div>
            </div>
        </form>
    </div>
</div>
                </div>
                <div class="col-xs-6 pull-right logged-user hidden">
                    <ul class="logged navbar-toggle inline-list pull-right navbar-mobile" data-toggle="collapse" data-target="#LefttHeader">

    <li>
        <div id="myfrontierssponsive" class="clickable profile-dropdown-responsive hidden">
            <a class="myfrontiersmobile">
                <p class="frontiers-letter">
                    Frontiers
                </p>
            </a>
        </div>
    </li>
    <li class="myofficecontainer">
        <div id="myoffice" class="clickable profile-dropdown-responsive">
            <p class="frontiers-letter">
                Frontiers
            </p>
            <em class="down-arrow-rhs"></em>
        </div>

        <div id="myofficeresponsive" class="clickable profile-dropdown-responsive hidden">
            <p class="frontiers-letter">
                Office
            </p>
            <em class="down-arrow-rhs"></em>
        </div>
    </li>
    <li class="no-left-margin profile-container-responsive">
        <a href="javascript:void(0)">
            <img class="profile-pic responsive lo_sensitive">
        </a>
    </li>

</ul>

<ul class="logged inline-list pull-right navbar-collapse collapse navbar-desktop">
    <li class="myfrontiers-container">
        <a>
            <span>My frontiers</span>
            <em></em>
        </a>
    </li>

    <li class="myfrontiersbeta-container hidden">
        <a class="myfrontiersbeta">
            <span>My Frontiers</span>
        </a>
    </li>

    <li class="myoffice-container hidden">
        <a>
            <span>Office</span>
            <em></em>
        </a>
    </li>

    <li class="myhome-container">
        <a class="myhome" title="My Home" data-placement="left" data-header-tracking='{ "category": "Header_Action", "action": "link", "label": "click_header_loop" }'>
            <i class="fromSprite loop-logo"></i>
        </a>
    </li>
    <li class="no-left-margin impact-container">
        <a class="popover-impact-disabled" title="My Impact" data-placement="left" data-header-tracking='{ "category": "Header_Action", "action": "link", "label": "click_header_impact" }'>
            <i class="fromSprite impactLogo"></i>
        </a>
    </li>
    
    <li class="no-left-margin username-container">
        <a class="username lo_sensitive" data-header-tracking='{ "category": "Header_Action", "action": "link", "label": "click_header_user" }'></a>
    </li>
    <li class="no-left-margin profile-container">
        <a class="popover-profile-trigger">
            <img class="profile-pic desktop lo_sensitive">
        </a>
    </li>
</ul>

<div class="popover-wrapper popover-impact" style="display: none">
    <em></em>
    <div class="popover-menu impact-popover"></div>
</div>

<div class="popover-wrapper popover-notification" style="display: none">
    <em></em>
    <input id="hdnLastNotificationId" type="hidden" />
    <div class="popover-menu notifications-popover"></div>
</div>

                </div>
            </div>
        </div>
    </div>
</div>





    

    <!--[if lte IE 8]>
        <div class="row">
            <div class="col-md-16">
                <div style="margin-bottom: 15px;"><h2>Warning!</h2><p>You are using an <strong>outdated</strong> browser. This page doesn't support Internet Explorer 6, 7 and 8.<br />Please <a class="blue" href="http://browsehappy.com/">upgrade your browser</a> or <a class="blue" href="http://www.google.com/chromeframe/?redirect=true">activate Google Chrome Frame</a> to improve your experience.</p></div>
            </div>
        </div>
    <![endif]-->

    <div class="page-container">
        

<header>

            <a href="https://blog.frontiersin.org/2018/07/02/impact-factor-scientific-academic-journal-ranking-report/?utm_source=FWEB&amp;utm_medium=FJOUR&amp;utm_campaign=IF18_GEN_TOP" target="_blank">
            <div class="marketing no-impact">
                


                <span class="message">
                    <p><span style="font-family: Calibri, 'Segoe UI', Calibri, Thonburi, Arial, Verdana, sans-serif, 'Mongolian Baiti', 'Microsoft Yi Baiti', 'Javanese Text'; font-size: 13.3333px; white-space: pre-wrap;">Frontiers journals are at the <strong>top of citation and impact metrics</strong></span></p>
                </span>

            </div>
        </a>
<div id="journal-header" class="journal-home" style="position: relative;">
    <div class="container-fluid main-container-xxl">
        <table id="logo-table" style="width:100%">
            <tr>
                    <td width="50%" valign="bottom" align="right" class="left-cell">
                        <h1>
                            <a href="https://www.frontiersin.org/journals/657" data-test-id="journal-link">
                                <span>Frontiers </span>in Robotics<br/>and AI
                            </a>
                        </h1>
                    </td>
                        <td class="right-cell" width="50%" style="vertical-align: bottom">
                            <h2>
                                <a href="https://www.frontiersin.org/journals/657/sections/1186" data-test-id="section-link"  class="btn btn-link">
                                    Robot and Machine Vision


                                </a>
                            </h2>
                        </td>
            </tr>
        </table>
    </div>
</div>


    <div id="journal-nav">
    <div class="container-fluid main-container-xxl">
        <div class="row">
            <div class="col-md-12">
                <div class="nav-wrapper">
                    <nav class="navbar navbar-default">
                        <!-- Brand and toggle get grouped for better mobile display -->
                        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#journal-navbar-collapse">
                            <span class="sr-only">Toggle navigation</span>
                            <span class="pull-left">
                                <span class="icon-bar"></span>
                                <span class="icon-bar"></span>
                                <span class="icon-bar"></span>
                            </span>
                            <span class="pull-right hidden-xs">Section</span>
                        </button>
                        <!-- Collect the nav links, forms, and other content for toggling -->
                        <div class="collapse navbar-collapse no-transition journal-nav-bar" id="journal-navbar-collapse">
                            <ul class="nav navbar-nav">
                                <li class="tab-overview icon icon-left">
                                    <a href="https://www.frontiersin.org/journals/robotics-and-ai/sections/robot-and-machine-vision#"><span class="sr-only">(current)</span><i class="fa fa-home"></i>Section</a>
                                </li>
                                <li class="tab-about">
                                    <a href="https://www.frontiersin.org/journals/robotics-and-ai/sections/robot-and-machine-vision#about">About</a>
                                </li>
                                <li class="tab-archive active">
                                    <a href="https://www.frontiersin.org/journals/robotics-and-ai/sections/robot-and-machine-vision#articles">Articles</a>
                                </li>
                                <li class="tab-research-topics">
                                    <a href="https://www.frontiersin.org/journals/robotics-and-ai/sections/robot-and-machine-vision#research-topics">Research topics</a>
                                </li>
                                <li class="dropdown">
                                    <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">For authors <span class="caret"></span></a>
                                    <ul class="dropdown-menu" role="menu">
                                        <li><a href="https://www.frontiersin.org/journals/robotics-and-ai/sections/robot-and-machine-vision#why-submit">Why submit?</a></li>
                                        <li><a href="https://www.frontiersin.org/about/publishing-fees" target="_blank">Fees</a></li>
                                        <li><a href="https://www.frontiersin.org/journals/robotics-and-ai/sections/robot-and-machine-vision#article-types">Article types</a></li>
                                        <li><a href="https://www.frontiersin.org/journals/robotics-and-ai/sections/robot-and-machine-vision#author-guidelines">Author guidelines</a></li>
                                        <li><a href="https://www.frontiersin.org/journals/robotics-and-ai/sections/robot-and-machine-vision#review-guidelines">Review guidelines</a></li>
                                        <li><a href="https://www.frontiersin.org/journals/robotics-and-ai/sections/robot-and-machine-vision#submission-checklist">Submission checklist</a></li>
                                        <li><a href="https://www.frontiersin.org/journals/robotics-and-ai/sections/robot-and-machine-vision#contact-editorial-office">Contact editorial office</a></li>
                                            <li><a href="https://www.frontiersin.org/submission/submissioninfo.aspx" class="action-link">Submit your manuscript</a></li>
                                    </ul>
                                </li>
                                <li class="tab-editorial-board">
                                    <a href="https://www.frontiersin.org/journals/robotics-and-ai/sections/robot-and-machine-vision#editorial-board">Editorial board</a>
                                </li>
                            </ul>
                        </div><!-- /.navbar-collapse -->
                        <ul class="list-inline follow-us nav navbar-nav">
                            <li class="twitter icon button">
                                <a data-test-id="twitter-link" href="https://twitter.com/@FrontRoboticsAI">
                                    <i class="fa fa-twitter"></i>
                                </a>
                            </li>
                            <li class="rss icon button">
                                <a data-test-id="rss-link" href="https://www.frontiersin.org/journals/robotics-and-ai/sections/robot-and-machine-vision/rss">
                                    <i class="fa fa-rss"></i>
                                </a>
                            </li>
                            <li class="alerts icon button">
                                <a data-test-id="alert-link" href="http://connect.frontiersin.org/subscriptions/subscribe?item=1&amp;field=72">
                                    <i class="fa fa-bell" style="width: auto; padding-right: 7px; padding-left: 7px;"><span class="article-alert-text" style="font-family: MuseoSans, 'Helvetica Neue', Helvetica, Arial, sans-serif; margin-left: 6px; font-weight: 700; letter-spacing: 0px; font-size: 13px;">Article alerts</span></i>
                                </a>
                            </li>
                                    
                            
                        </ul>

                    </nav>
                </div><!-- / .nav-wrapper -->
            </div>
        </div>
    </div>
</div>

</header>

<div class="journal-actions article-actions">
    <div class="container-fluid main-container-xxl">
        <div class="row">
            <div class="col-lg-6 col-md-6 col-lg-push-6 col-md-push-6 col-research-topic">
            </div>
            <section class="col-lg-4 col-lg-offset-2 col-md-6 col-md-offset-0 col-lg-pull-6 col-md-pull-6">
                <a href="https://www.frontiersin.org/journals/657/sections/1186#articles" class="archive-link hidden-sm hidden-xs" data-test-id="archive-link">
                    <i class="fa fa-angle-left"></i> Articles
                </a>
            </section>
        </div>
    </div>
</div>


<aside id="anchors" class="pull-left table-of-contents side-article">
    <div class="side-people hidden-sm hidden-xs">
                <section class="side-article-editors">
                    <header><h5 class="like-h4">Edited by</h5></header>
                    <a class="authors" href="https://loop.frontiersin.org/people/279005/overview">
                        <img alt=" " class="pr5 pull-left" src="https://loop.frontiersin.org/images/profile/279005/24"></img>
                        <h6 class="author-link-aside">Trung Dung Ngo</h6>
                    </a>
                    <div class="clearfix"></div>
                    <p><span class="notes">Universiti Brunei Darussalam, Brunei</span></p>
                </section>
                <section class="side-article-editors">
                    <header><h5 class="like-h4"> Reviewed by</h5></header>
                            <a class="authors" href="https://loop.frontiersin.org/people/59742/overview">
                                <img alt=" " class="pr5 pull-left" src="https://loop.frontiersin.org/images/profile/59742/24"></img>
                                <h6 class="author-link-aside">Fady Alnajjar</h6>
                            </a>
                            <div class="clearfix"></div>
                        <p><span class="notes">United Arab Emirates University, United Arab Emirates</span></p>
                            <a class="authors" href="https://loop.frontiersin.org/people/427082/overview">
                                <img alt=" " class="pr5 pull-left" src="https://loop.frontiersin.org/images/profile/427082/24"></img>
                                <h6 class="author-link-aside">Yongsheng Ou</h6>
                            </a>
                            <div class="clearfix"></div>
                        <p><span class="notes">Shenzhen Institutes of Advanced Technology (CAS), China</span></p>
                    <p class="disclaimer">The editor and reviewers' affiliations are the latest provided on their Loop research profiles and may not reflect their situation at the time of review.</p>
                </section>
    </div>
    <nav>
            <header><h5 class="like-h4" style="padding-top: 14px; padding-left: 6px; margin-bottom: 6px;">TABLE OF CONTENTS</h5></header>
            <ul class="nav nav-list list-unstyled contents">
                <li>
<ul class="flyoutJournal">
<li><a href="#h1">Abstract</a></li>
<li><a href="#h2">1. Introduction</a></li>
<li><a href="#h3">2. System Overview</a></li>
<li><a href="#h4">3. Interfaces</a></li>
<li><a href="#h5">4. Evaluation</a></li>
<li><a href="#h6">5. Evaluation Results and Discussion</a></li>
<li><a href="#h7">6. Conclusion</a></li>
<li><a href="#h8">Author Contributions</a></li>
<li><a href="#h9">Conflict of Interest Statement</a></li>
<li><a href="#h10">Funding</a></li>
<li><a href="#h11">References</a></li>
</ul>                </li>
            </ul>
    </nav>
</aside>


<div id="article" class="boxed white no-padding">
    <div class="container-fluid main-container-xxl">
            <div class="side-article-mrk hidden-md hidden-lg top clearfix">
                <style type="text/css"><!--
.spotlight-title:hover,
.spotlight-title:focus {
 color: #555 !important; 
}
.spotlight-link {
 color: #f2b12f !important;
}
.spotlight-link:hover,
.spotlight-link:focus {
 color: #e5a812 !important;
}
--></style>
<div class="spotlight"><a href="https://spotlight.frontiersin.org/how-to-enter/?utm_source=FWEB&amp;utm_medium=FMAIN&amp;utm_campaign=SPOTLIGHT&amp;utm_term=BANNER&amp;utm_content=WEB"><img src="https://3718aeafc638f96f5bd6-d4a9ca15fc46ba40e71f94dec0aad28c.ssl.cf1.rackcdn.com/article-marketing-message-1f3be380-af93-4b04-a218-32e22eb3bdda.png" alt="" /></a><br />
<h3 style="font-weight: 500;"><a class="spotlight-title" tabindex="0" href="https://spotlight.frontiersin.org/how-to-enter/?utm_source=FWEB&amp;utm_medium=FMAIN&amp;utm_campaign=SPOTLIGHT&amp;utm_term=BANNER&amp;utm_content=WEB" target="_blank">Want to win $100,000 to host your own conference?</a></h3>
<a class="spotlight-link" href="https://spotlight.frontiersin.org/how-to-enter/?utm_source=FWEB&amp;utm_medium=FMAIN&amp;utm_campaign=SPOTLIGHT&amp;utm_term=BANNER&amp;utm_content=WEB" target="_blank">Suggest a Research Topic</a></div>
            </div>
        <div class="row">

            
<aside class="a col-xs-12 col-sm-12 col-lg-2 col-md-3 pull-right side-article right-container">
        <button type="button" class="navbar-toggle navbar-download collapsed" data-toggle="collapse" data-target=".show" aria-expanded="false">
            <span class="icon"><i class="fa fa-download"></i></span>
        </button>
    <button type="button" class="navbar-toggle navbar-share collapsed" data-toggle="collapse" data-target=".show" aria-expanded="false">
        <span class="icon"><i class="fa fa-share-alt"></i></span>
    </button>
        <div class="side-article-download show collapse clearfix">
            <ul class="list-unstyled list-inline clearfix">
                    <li class="dropdown text-center paper">
                        <a data-target="#" class="dropdown-toggle" data-test-id="download-button" data-toggle="dropdown" role="button" aria-expanded="false">
                            <span class="icon icon-article"><i class="fa fa-file-pdf-o"></i></span>
                            <span class="icon-label">Download Article</span>
                        </a>
                        <ul class="dropdown-menu" role="menu">
                                <li>
                                    <a class="download-files-pdf action-link" href="/articles/10.3389/frobt.2017.00020/pdf" data-test-id="article-downloadpdf">
                                        Download PDF
                                    </a>
                                </li>
                                <li>
                                    <a class="download-files-readcube" href="http://www.readcube.com/articles/10.3389/frobt.2017.00020" data-test-id="article-viewenhancedpdf">
                                        ReadCube
                                    </a>
                                </li>
                                <li>
                                    <a class="download-files-epub" href="/articles/10.3389/frobt.2017.00020/epub" data-test-id="article-epub">
                                        EPUB
                                    </a>
                                </li>
                                <li>
                                    <a class="download-files-nlm" href="/articles/10.3389/frobt.2017.00020/xml/nlm" data-test-id="article-nlm">
                                        XML (NLM)
                                    </a>
                                </li>
                                                            <li class="disable">

                                        <span class="supplemental-data-disabled">
                                            Supplementary
                                            <br>
                                            Material
                                        </span>

                                </li>
                        </ul>
                    </li>
                <li class="dropdown text-center citation">
                    <a data-target="#" class="dropdown-toggle" data-test-id="citation-button" data-toggle="dropdown" role="button" aria-expanded="false">
                        <span class="icon icon-citation"><i class="fa fa-quote-left"></i></span>
                        <span class="icon-label">Export citation</span>
                    </a>
                    <ul class="dropdown-menu" role="menu">
                            <li>
                                <a data-test-id="article-endnote" href="https://www.frontiersin.org/articles/10.3389/frobt.2017.00020/endNote">
                                    EndNote
                                </a>
                            </li>
                            <li>
                                <a data-test-id="article-referencemanager" href="https://www.frontiersin.org/articles/10.3389/frobt.2017.00020/reference">
                                    Reference Manager
                                </a>
                            </li>
                            <li>
                                <a data-test-id="article-simpletextfile" href="https://www.frontiersin.org/articles/10.3389/frobt.2017.00020/text">
                                    Simple TEXT file
                                </a>
                            </li>
                            <li>
                                <a data-test-id="article-bibtex" href="https://www.frontiersin.org/articles/10.3389/frobt.2017.00020/bibTex">
                                    BibTex
                                </a>
                            </li>
                    </ul>
                </li>
            </ul>
        </div>
    <div class="side-article-impact">
        <ul class="nav">
            <li class="impact-data" title="The total view count is updated once a day, so not to worry if you don&#39;t see immediate results.">
                <span class="title-number"></span>
                <span class="title-text">total views</span>
            </li>
            <li class="hidden-sm hidden-xs">
                <div class="altmetric-icon">
                    <div class='altmetric-embed' data-badge-type='1' data-doi='10.3389/frobt.2017.00020' data-link-target="new"></div>
                </div>
            </li>
        </ul>
            <a type="button" class="btn btn-default hidden-sm hidden-xs btn-impact " data-test-id="view-article-impact" href="http://loop-impact.frontiersin.org/impact/article/238204#views" target="_blank">
                <span class="icon-impact"><i class="fa fa-line-chart"></i></span> View Article Impact
            </a>            
    </div>
        <div class="side-article-mrk hidden-sm hidden-xs clearfix">
            <style type="text/css"><!--
.spotlight-title:hover,
.spotlight-title:focus {
 color: #555 !important; 
}
.spotlight-link {
 color: #f2b12f !important;
}
.spotlight-link:hover,
.spotlight-link:focus {
 color: #e5a812 !important;
}
--></style>
<div class="spotlight"><a href="https://spotlight.frontiersin.org/how-to-enter/?utm_source=FWEB&amp;utm_medium=FMAIN&amp;utm_campaign=SPOTLIGHT&amp;utm_term=BANNER&amp;utm_content=WEB"><img src="https://3718aeafc638f96f5bd6-d4a9ca15fc46ba40e71f94dec0aad28c.ssl.cf1.rackcdn.com/article-marketing-message-1f3be380-af93-4b04-a218-32e22eb3bdda.png" alt="" /></a><br />
<h3 style="font-weight: 500;"><a class="spotlight-title" tabindex="0" href="https://spotlight.frontiersin.org/how-to-enter/?utm_source=FWEB&amp;utm_medium=FMAIN&amp;utm_campaign=SPOTLIGHT&amp;utm_term=BANNER&amp;utm_content=WEB" target="_blank">Want to win $100,000 to host your own conference?</a></h3>
<a class="spotlight-link" href="https://spotlight.frontiersin.org/how-to-enter/?utm_source=FWEB&amp;utm_medium=FMAIN&amp;utm_campaign=SPOTLIGHT&amp;utm_term=BANNER&amp;utm_content=WEB" target="_blank">Suggest a Research Topic</a></div>
        </div>
    <div class="side-article-share show collapse">        
        <h5 class="like-h4">SHARE ON</h5>
        <ul class="share-media clearfix" style="padding: 0;">
            <li class="social_block">
                <div class="atButton">
                    <a class="addthis_button_facebook at300b" addthis:title="Mobile Mixed-Reality Interfaces That Enhance Human–Robot Interaction in Shared Spaces" addthis:description="Although user interfaces with gesture-based input and augmented graphics have promoted intuitive human-robot interactions (HRI), they are often implemented in remote applications on research-grade platforms requiring significant training and limiting  operator mobility. This paper proposes a mobile mixed-reality interface approach to enhance HRI in shared spaces. As a user points a mobile device at the robot's workspace, a mixed-reality environment is rendered providing a common frame of reference for the user and robot to effectively communicate spatial information for performing object manipulation tasks, improving the user's situational awareness while interacting with augmented graphics to intuitively command the robot. An evaluation with participants is conducted to examine task performance and user experience associated with the proposed interface strategy in comparison to conventional approaches that utilize egocentric or exocentric views from cameras mounted on the robot or in the environment, respectively. Results indicate that, despite the suitability of the conventional approaches in remote applications, the proposed interface approach provides comparable task performance and user experiences in shared spaces without the need to install operator stations or vision systems on or around the robot. Moreover, the proposed interface approach provides users the flexibility to direct robots from their own visual perspective (at the expense of some physical workload) an..." addthis:url="https://www.frontiersin.org/article/10.3389/frobt.2017.00020" title="Facebook"></a>
                </div>
                <a><span class="facebook_count"></span></a>
            </li>
            <li class="social_block">
                <div class="atButton">
                    <a class="addthis_button_twitter at300b" addthis:title="Mobile Mixed-Reality Interfaces That Enhance Human–Robot Interaction in Shared Spaces" addthis:description="Although user interfaces with gesture-based input and augmented graphics have promoted intuitive human-robot interactions (HRI), they are often implemented in remote applications on research-grade platforms requiring significant training and limiting  operator mobility. This paper proposes a mobile mixed-reality interface approach to enhance HRI in shared spaces. As a user points a mobile device at the robot's workspace, a mixed-reality environment is rendered providing a common frame of reference for the user and robot to effectively communicate spatial information for performing object manipulation tasks, improving the user's situational awareness while interacting with augmented graphics to intuitively command the robot. An evaluation with participants is conducted to examine task performance and user experience associated with the proposed interface strategy in comparison to conventional approaches that utilize egocentric or exocentric views from cameras mounted on the robot or in the environment, respectively. Results indicate that, despite the suitability of the conventional approaches in remote applications, the proposed interface approach provides comparable task performance and user experiences in shared spaces without the need to install operator stations or vision systems on or around the robot. Moreover, the proposed interface approach provides users the flexibility to direct robots from their own visual perspective (at the expense of some physical workload) an..." addthis:url="https://www.frontiersin.org/article/10.3389/frobt.2017.00020" title="Tweet"></a>
                </div>
                <a><span class="twitter_count"></span></a>
            </li>
            <li class="social_block">
                <div class="atButton">
                    <a class="addthis_button_google at300b" addthis:title="Mobile Mixed-Reality Interfaces That Enhance Human–Robot Interaction in Shared Spaces" addthis:description="Although user interfaces with gesture-based input and augmented graphics have promoted intuitive human-robot interactions (HRI), they are often implemented in remote applications on research-grade platforms requiring significant training and limiting  operator mobility. This paper proposes a mobile mixed-reality interface approach to enhance HRI in shared spaces. As a user points a mobile device at the robot's workspace, a mixed-reality environment is rendered providing a common frame of reference for the user and robot to effectively communicate spatial information for performing object manipulation tasks, improving the user's situational awareness while interacting with augmented graphics to intuitively command the robot. An evaluation with participants is conducted to examine task performance and user experience associated with the proposed interface strategy in comparison to conventional approaches that utilize egocentric or exocentric views from cameras mounted on the robot or in the environment, respectively. Results indicate that, despite the suitability of the conventional approaches in remote applications, the proposed interface approach provides comparable task performance and user experiences in shared spaces without the need to install operator stations or vision systems on or around the robot. Moreover, the proposed interface approach provides users the flexibility to direct robots from their own visual perspective (at the expense of some physical workload) an..." addthis:url="https://www.frontiersin.org/article/10.3389/frobt.2017.00020" title="Google"></a>
                </div>
                <a><span class="googleplus_count"></span></a>
            </li>
            <li class="social_block">
                <div class="atButton">
                    <a class="addthis_button_linkedin at300b" addthis:title="Mobile Mixed-Reality Interfaces That Enhance Human–Robot Interaction in Shared Spaces" addthis:description="Although user interfaces with gesture-based input and augmented graphics have promoted intuitive human-robot interactions (HRI), they are often implemented in remote applications on research-grade platforms requiring significant training and limiting  operator mobility. This paper proposes a mobile mixed-reality interface approach to enhance HRI in shared spaces. As a user points a mobile device at the robot's workspace, a mixed-reality environment is rendered providing a common frame of reference for the user and robot to effectively communicate spatial information for performing object manipulation tasks, improving the user's situational awareness while interacting with augmented graphics to intuitively command the robot. An evaluation with participants is conducted to examine task performance and user experience associated with the proposed interface strategy in comparison to conventional approaches that utilize egocentric or exocentric views from cameras mounted on the robot or in the environment, respectively. Results indicate that, despite the suitability of the conventional approaches in remote applications, the proposed interface approach provides comparable task performance and user experiences in shared spaces without the need to install operator stations or vision systems on or around the robot. Moreover, the proposed interface approach provides users the flexibility to direct robots from their own visual perspective (at the expense of some physical workload) an..." addthis:url="https://www.frontiersin.org/article/10.3389/frobt.2017.00020" title="LinkedIn"></a>
                </div>
                <a><span class="linkedin_count"></span></a>
            </li>
            <li class="social_block">
                <div class="atButton">
                    <a class="addthis_button_more at300b" addthis:title="Mobile Mixed-Reality Interfaces That Enhance Human–Robot Interaction in Shared Spaces" addthis:description="Although user interfaces with gesture-based input and augmented graphics have promoted intuitive human-robot interactions (HRI), they are often implemented in remote applications on research-grade platforms requiring significant training and limiting  operator mobility. This paper proposes a mobile mixed-reality interface approach to enhance HRI in shared spaces. As a user points a mobile device at the robot's workspace, a mixed-reality environment is rendered providing a common frame of reference for the user and robot to effectively communicate spatial information for performing object manipulation tasks, improving the user's situational awareness while interacting with augmented graphics to intuitively command the robot. An evaluation with participants is conducted to examine task performance and user experience associated with the proposed interface strategy in comparison to conventional approaches that utilize egocentric or exocentric views from cameras mounted on the robot or in the environment, respectively. Results indicate that, despite the suitability of the conventional approaches in remote applications, the proposed interface approach provides comparable task performance and user experiences in shared spaces without the need to install operator stations or vision systems on or around the robot. Moreover, the proposed interface approach provides users the flexibility to direct robots from their own visual perspective (at the expense of some physical workload) an..." addthis:url="https://www.frontiersin.org/article/10.3389/frobt.2017.00020" title="View more services"></a>
                </div>
                <a><span class="total_count"></span></a>
            </li>
        </ul>
    </div>
</aside>


            
<main class="b col-xs-12 col-sm-12 col-lg-8 col-lg-push-2 col-md-7 col-md-push-2 pull-left">
    <div class="article-section">
        <div class="article-container" data-html="True">
            <div class="abstract-container">
                
                <div class="article-header-container">
                    <!-- Start CrossMark Snippet v2.0 -->
                         <!-- the external CrossMark script to load the crossmark logo responsive  JPB-5562 -->
                        <script src="https://crossmark-cdn.crossref.org/widget/v2.0/widget.js"></script>
                        <a data-target="crossmark" class="crossmark pull-right" style="padding: 3px 0 13px 0;"><img id="crossmark-icon" style="border: 0; width:64px; height:64px;" src="https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg"  /></a>
                        <div id="crossmark-dialog" style="display: none;" title="">
                            <!-- the external CrossMark data is loaded inside this iframe -->
                            <iframe id="crossmark-dialog-frame" frameborder="0"></iframe>
                        </div>
                    <!-- End CrossMark Snippet -->
                    <div class="header-bar-one">
                      <h2>Original Research ARTICLE

                        </h2>
                    </div>
                    <div class="header-bar-three">
                        Front. Robot. AI, 09 June 2017
                                 | <a href="https://doi.org/10.3389/frobt.2017.00020">https://doi.org/10.3389/frobt.2017.00020</a>
                    </div>
                </div>
                
<div class="JournalAbstract">
<a id="h1"></a><h1>Mobile Mixed-Reality Interfaces That Enhance Human&#x02013;Robot Interaction in Shared Spaces</h1>
<div class="authors">
<a style="text-decoration:none;line-height:1.3em;" href="http://www.frontiersin.org/people/u/381577" class="user-id-381577"><img class="pr5" src="https://loop.frontiersin.org/images/profile/381577/24" onerror="this.src='https://f96a1a95aaa960e01625-a34624e694c43cdf8b40aa048a644ca4.ssl.cf2.rackcdn.com/Design/Images/newprofile_default_profileimage_new.jpg'" alt="image"/>Jared A. Frank</a>, 
<a style="text-decoration:none;line-height:1.3em;" href="http://www.frontiersin.org/people/u/381582" class="user-id-381582"><img class="pr5" src="https://loop.frontiersin.org/images/profile/381582/24" onerror="this.src='https://f96a1a95aaa960e01625-a34624e694c43cdf8b40aa048a644ca4.ssl.cf2.rackcdn.com/Design/Images/newprofile_default_profileimage_new.jpg'" alt="image"/>Matthew Moorhead</a> and <a style="text-decoration:none;line-height:1.3em;" href="http://www.frontiersin.org/people/u/386707" class="user-id-386707"><img class="pr5" src="https://loop.frontiersin.org/images/profile/386707/24" onerror="this.src='https://f96a1a95aaa960e01625-a34624e694c43cdf8b40aa048a644ca4.ssl.cf2.rackcdn.com/Design/Images/newprofile_default_profileimage_new.jpg'" alt="image"/>Vikram Kapila</a>*
</div>
<ul class="notes">
<li class="pl0">Mechatronics, Controls, and Robotics Laboratory, Mechanical and Aerospace Engineering Department, NYU Tandon School of Engineering, Brooklyn, NY, USA</li>
</ul>
<p>Although user interfaces with gesture-based input and augmented graphics have promoted intuitive human&#x02013;robot interactions (HRI), they are often implemented in remote applications on research-grade platforms requiring significant training and limiting operator mobility. This paper proposes a mobile mixed-reality interface approach to enhance HRI in shared spaces. As a user points a mobile device at the robot&#x02019;s workspace, a mixed-reality environment is rendered providing a common frame of reference for the user and robot to effectively communicate spatial information for performing object manipulation tasks, improving the user&#x02019;s situational awareness while interacting with augmented graphics to intuitively command the robot. An evaluation with participants is conducted to examine task performance and user experience associated with the proposed interface strategy in comparison to conventional approaches that utilize egocentric or exocentric views from cameras mounted on the robot or in the environment, respectively. Results indicate that, despite the suitability of the conventional approaches in remote applications, the proposed interface approach provides comparable task performance and user experiences in shared spaces without the need to install operator stations or vision systems on or around the robot. Moreover, the proposed interface approach provides users the flexibility to direct robots from their own visual perspective (at the expense of some physical workload) and leverages the sensing capabilities of the tablet to expand the robot&#x02019;s perceptual range.</p>
<div class="clear"></div>
</div>
<div class="JournalFullText">
<a id="h2"></a><h2>1. Introduction</h2>
<p class="mb15">While robotics technologies and applications have experienced accelerating advances, with robots vacuuming homes, assembling automobiles, exploring planets and oceans, and performing surgeries, their potential remains limited by their ability to effectively interact with people. Recent efforts have explored scenarios in which service robots function alongside people in shared workspaces (<a href="#B29">Shah and Breazeal, 2010</a>). To ensure the physical safety and psychological comfort of users in these interaction scenarios, a high level of mutual attention and awareness are required of the human and robot (<a href="#B4">Drury et al., 2003</a>). Efforts to maintain these conditions have typically been addressed in the design of the robot, with respect to its sensing (<a href="#B6">Fritzsche et al., 2011</a>), reasoning (<a href="#B19">McGhan et al., 2015</a>), and control (<a href="#B16">Lew et al., 2000</a>), as well as in the modification of the environment (<a href="#B15">Lenz et al., 2012</a>; <a href="#B23">Morato et al., 2014</a>) for direct physical HRI applications. However, graphical interfaces have begun to provide elegant solutions for interacting with home service robots (<a href="#B14">Lee et al., 2007</a>; <a href="#B27">Sakamoto et al., 2016</a>), drawing inspiration from interfaces used by trained engineers to remotely operate robotic vehicles and manipulators. Recent efforts have also sought new interface strategies that permit non-technical users to intuitively interact with robots by using non-verbal gestures captured through vision (<a href="#B32">Waldherr et al., 2000</a>), touch (<a href="#B21">Micire et al., 2009</a>), and inertial sensing (<a href="#B13">Kao and Li, 2010</a>). However, current implementations often confine users to computer stations that can be costly, limited in mobility, or contain hardware and software that are unfamiliar or inconvenient for lay users. For seamless interaction with robots in shared spaces, readily accessible mobile solutions offer a compelling opportunity.</p>
<p class="mb15">Recent advances in mobile technologies have allowed state-of-the-art features like image processing, multitouch gesture detection, device attitude estimation, and 3D graphics rendering to be integrated on mobile devices such as smartphones and tablet computers to provide portable interfaces for enhanced HRI. Moreover, with their familiarity and ease of use, mobile devices can support intuitive applications to operate robotic systems with comparable performance and usability <i>vis-a-vis</i> conventional research-grade interfaces, for a fraction of the cost and training (<a href="#B31">Su et al., 2015</a>). Although the principles of efficient HRI have begun to be implemented in the design of mobile interfaces, such as those that enable shared or adjustable autonomy in interactions with service robots (<a href="#B24">Muszynski et al., 2012</a>; <a href="#B1">Birkenkampf et al., 2014</a>), they have principally been investigated in remote operations rather than in shared human&#x02013;robot spaces. This presents a problem, since interaction techniques utilized in teleoperation are not necessarily directly applicable to interactions in close quarters.</p>
<p class="mb15">In teleoperation scenarios, perception of the remote environment often relies on visual feedback provided from either egocentric perspectives using cameras mounted on the robot (<a href="#B20">Menchaca-Brandan et al., 2007</a>) or exocentric perspectives using cameras mounted in the environment (<a href="#B11">Hashimoto et al., 2011</a>). Several usability issues have been associated with these perspectives that can compromise the operator&#x02019;s situational awareness. For example, the limited field of view of cameras can cause a keyhole effect, in which operators miss important events that occur offscreen (<a href="#B33">Woods et al., 2004</a>), requiring them to consult additional sensor information and store mental models of the remote environment in short-term memory (both of which introduce significant cognitive workload) to maintain situational awareness (<a href="#B7">Goodrich and Olsen, 2003</a>). Moreover, latency and low image quality as a result of limited bandwidth, as well as orientation and frame of reference issues that stem from unnatural camera viewpoints, can disrupt situational awareness and sensations of telepresence (<a href="#B3">Chen et al., 2007</a>). Proposed solutions to these challenges have included training operators to utilize multiple, multimodal, or ecological displays and controls (<a href="#B12">Hughes and Lewis, 2005</a>; <a href="#B18">Mar&#x000ED;n et al., 2005</a>; <a href="#B25">Nielsen et al., 2007</a>; <a href="#B8">Green et al., 2008a</a>). Moreover, virtual and augmented reality have played a fundamental role in recent efforts, providing environments that can serve as effective tools for spatial communication (<a href="#B9">Green et al., 2008b</a>) and visualizations that promote situational awareness despite limited access to on-site cameras, suboptimal positioning of cameras, poor viewing angles, occlusion, etc. (<a href="#B34">Ziaei et al., 2011</a>). Specifically, <a href="#B22">Milgram et al. (1993)</a> have examined the use of gesture-based interactions with virtual objects as a means to communicate spatial information to robots to command and control their performance of tasks with physical objects.</p>
<p class="mb15">Many of the usability issues encountered with conventional egocentric and exocentric visual perspectives in teleoperation scenarios can be compounded when used in shared spaces. For example, using robot-mounted cameras to render egocentric perspectives can cause a keyhole effect, while exocentric views require the installation of cameras in the environment. With either perspective, users can experience orientation and frame of reference issues as they attempt to adapt to a different visual perspective from their own. When users are collocated with the robot, they may tend to switch their attention between the interface screen, which presents one visual perspective, and their direct perspective of the robot, which can lead to increased mental workload (<a href="#B3">Chen et al., 2007</a>). Moreover, computational overhead associated with encoding and streaming video to the mobile device will introduce latency that can obstruct performance and user comfort. By leveraging the mobility, computational power, and embedded sensors of the interface device, many of these usability issues can be avoided.</p>
<p class="mb15">This paper aims to investigate how a mobile mixed-reality interface approach implemented on mobile devices can enhance the interactive communication of spatial information with robots for object manipulation in shared spaces. Rather than limiting the user experience with unfamiliar, uncomfortable, and expensive research-grade equipment, the mobility of smartphones and tablets affords operators the flexibility of using an interface while in close proximity to the robot. By utilizing the video of the shared workspace from the device&#x02019;s rear-facing camera, a number of benefits emerge over conventional approaches. Orientation and frame of reference issues are averted since visual feedback is presented from the perspective of the user. Thus, the device screen can act as an interactive window that allows users to command the robot by interacting directly with the world, thus reducing the cognitive load associated with attention switching. Moreover, latency arising from encoding and streaming video from robot-mounted or environment-mounted cameras is eliminated, increasing responsiveness of the interface.</p>
<p class="mb15">By implementing user interaction as well as robot perception, reasoning, and control in the user&#x02019;s frame of reference, a common ground is established between the user and robot that facilitates mutual awareness through information sharing. This information sharing can be used to improve situational awareness by granting users intuitive visual access to information, such as the state of the robot or status of a task, through augmented graphics. Since the shared visual perspective is mobile, it is able to capture regions of the workspace that are undetectable by conventional egocentric and exocentric views. Thus, the proposed mobile mixed-reality interface approach can be used to provide the robot with spatial information that can enhance and in some cases replace the information from the robot while planning and controlling its motion, due to sensorial and mechanical limitations of the robot (constraints on the field of view of its cameras or on the reachable space of its limbs).</p>
<p class="mb0">The paper is organized as follows. In Sections 2 and 3, the materials and methods used in this study are described, respectively. Section 2 introduces the robot, the workspace, and interface device technology used in the design of the study. Then, Section 3 presents the three mobile human&#x02013;robot interface design strategies investigated in the study. Section 4 outlines the user evaluation conducted to compare the three interface strategies under investigation, including the experimental procedure, the task performed by participants, and the assessment and analysis methods utilized. Section 5 discusses the results of the evaluation, while Section 6 provides concluding remarks and future directions of the research.</p>
<a id="h3"></a><h2>2. System Overview</h2>
<p class="mb0">The system used in this study consists of a humanoid robotic platform with two 6 degree-of-freedom (DOF) arms, a table with an assortment of blocks of different colors, a computer station for video recording and streaming, and a tablet device that is held by the user and provides a mixed-reality interface for interacting with the robot, as shown in Figure <a href="#F1">1</a>. The hardware and software architectures employed in this study are adapted from the study of <a href="#B5">Frank et al. (2016)</a>, in which the feasibility, precision, and user experience associated with mobile mixed-reality interfaces for HRI were recently investigated.</p>
<div class="DottedLine"></div>
<div class="Imageheaders">FIGURE 1</div>
<div class="FigureDesc">
<a href="https://www.frontiersin.org/files/Articles/238204/frobt-04-00020-HTML/image_m/frobt-04-00020-g001.jpg" target="_blank">
<img src="https://www.frontiersin.org/files/Articles/238204/frobt-04-00020-HTML/image_t/frobt-04-00020-g001.gif" id="F1" alt="www.frontiersin.org" /></a>
<p><strong>Figure 1. The environment used to conduct the human&#x02013;robot interaction study</strong>.</p>
</div>
<div class="clear"></div>
<div class="DottedLine"></div>
<h3>2.1. Robot</h3>
<p class="mb0">The robotic platform used in this study is a humanoid with two 6-DOF arms that can manipulate objects on a table. The robot is programmed to receive, over Wi-Fi, commands that contain desired poses for the robot&#x02019;s tool to pick up and place objects in its workspace. First, an algorithm determines which of the robot&#x02019;s arms to use to pick up and place the object. Then, an inverse kinematic model computes the angles required to orient the arm&#x02019;s joints such that its tool is brought to the desired pose. By constraining the tool to be oriented downward in a vertical plane, the robot&#x02019;s 6-DOF inverse kinematic model is decoupled into two 3-DOF solutions known as the inverse position kinematics and the inverse orientation kinematics of the arm (<a href="#B30">Spong et al., 2006</a>). The inverse position kinematics give the joint angles necessary to position the wrist center of the arm, and the inverse orientation kinematics give the joint angles necessary to orient the tool. An algorithm plans a path to complete the object manipulation task by generating a sequence of intermediate poses for the tool. Additional information regarding the robot&#x02019;s kinematics and path planning is provided in the study of <a href="#B5">Frank et al. (2016)</a>. The robot also has a stereoscopic vision system that consists of two small webcams mounted rigidly to a pan-and-tilt platform. In this study, the video captured by this system is utilized for HRI in only one of the three interface designs, <i>viz</i>., the one using an egocentric perspective.</p>
<h3>2.2. Workspace</h3>
<p class="mb0">The robot is surrounded by a symmetrical workspace composed of two rectangular sections, one on each side of the robot and one semicircular section in front of the robot (Figure <a href="#F2">2</a>). Directly in front of the robot on the semicircular section are three containers, a red, a green, and a blue. On each of the three sections of the workspace is a colored square block that must be picked up and placed by the robot into the container of corresponding color. On the semicircular section is a green block, on the rectangular section on the robot&#x02019;s left side is a red block, and on the rectangular region on the robot&#x02019;s right side is a blue block. Affixed to the surfaces of the workspace are twelve visual markers, which are arranged in a known pattern at predefined locations (Figure <a href="#F2">2</a>). These markers are used by the interface to establish a shared reference frame for the exchange of spatial information between the operator and the robot, as well as for the realistic display of augmented graphics. To facilitate the localization of the blocks with respect to this reference frame, markers are also affixed to each of the blocks. Each of the fifteen markers used in this study contains a distinct pattern that allows the marker to be uniquely identified. Mounted to the ceiling directly above the workspace is a camera, which is used to provide an overhead view. The video captured by this camera is recorded for all experimental trials performed in the study, but is utilized for HRI in only one of the three interface designs, <i>viz</i>., the one using an exocentric perspective.</p>
<div class="DottedLine"></div>
<div class="Imageheaders">FIGURE 2</div>
<div class="FigureDesc">
<a href="https://www.frontiersin.org/files/Articles/238204/frobt-04-00020-HTML/image_m/frobt-04-00020-g002.jpg" target="_blank">
<img src="https://www.frontiersin.org/files/Articles/238204/frobt-04-00020-HTML/image_t/frobt-04-00020-g002.gif" id="F2" alt="www.frontiersin.org" /></a>
<p><strong>Figure 2. Workspace markers arranged in a predefined pattern (as captured from the overhead camera)</strong>.</p>
</div>
<div class="clear"></div>
<div class="DottedLine"></div>
<h3>2.3. Computer Station and Interface Device</h3>
<p class="mb0">To record and stream video from the overhead and robot-mounted cameras, a computer station is installed beside the robot&#x02019;s workspace. The interface device accesses these video streams in two of the three designs (i.e., the ones that use the egocentric and exocentric perspectives) by connecting to the station with a client&#x02013;server architecture. The interface device used in conducting this study is an Apple iPad Pro. Released in 2016, this tablet computer has a 9.7&#x02033; (250 mm) screen with a 2,048 &#x000D7; 1,536 pixel multitouch display (264 pixels per inch), a 12-megapixel 4 K resolution rear-facing camera, 256 GB of flash memory, a 2.16 GHz dual-core processor, and 4 GB of RAM. These specifications make the iPad Pro, and the generations of mobile devices to come, uniquely suited to provide mobile mixed-reality human&#x02013;robot interfaces. This is so because these devices are capable of capturing and processing live video sequences, rendering 3D augmented graphics, recognizing multitouch gestures, estimating device attitude, and communicating over wireless networks, synchronously and in real time. Thus, these devices can provide more than intuitive user interfaces; running in the background, their mobile applications have the ability to offload some of the sensing, storage, and computation of the system. This ability is exemplified in one of the three interface designs explored in this study, <i>viz</i>., the one using the rear-facing camera of the tablet to provide a mobile perspective for HRI.</p>
<a id="h4"></a><h2>3. Interfaces</h2>
<p class="mb0">To explore mixed-reality as an enabler of HRI and investigate ways in which mobile technologies may enhance HRI in shared spaces, three mobile interfaces are developed that utilize distinct design strategies. In the front end, each interface provides a mixed-reality view of the shared space that consists of a live video of the space augmented with computer-generated graphics, such as reference planes and axes registered in the view and virtual objects linked to corresponding physical objects in the space. The principal difference between the three alternative interface designs lies in the visual perspective used to render the mixed-reality environment. In the first interface, the perspective comes from a camera mounted to the robot. In the second interface, the perspective comes from a camera mounted on the ceiling above the workspace. In the third interface, the perspective comes from the rear-facing camera on board the interface device. All three interfaces provide the same augmented graphics responsible for enhanced visual feedback to promote situational awareness and intuitive commanding of the robot through touch interactions with the graphics. For example, interactive virtual blocks are projected on top of actual blocks detected in the video. Through taps, drags, and rotations of the user&#x02019;s fingers on the touchscreen, the virtual blocks can be intuitively manipulated to desired poses on the screen. A virtual grid is projected onto the surface of the table to provide users with a visual aid for precise placement of the virtual blocks in the mixed-reality workspace. However, due to the limited lengths of the robot&#x02019;s arms and the constraints on their configurations imposed by the grasping approach taken by the robot, not all locations in the workspace can be reached. To inform themselves of the allowable and prohibited regions in the workspace, users can double tap on the screen to toggle on and off the display of a semitransparent cross-sectional plot of the robot&#x02019;s reachable space that is projected with the grid (see Figure <a href="#F3">3</a>). In this plot, red indicates prohibited regions while green indicates allowable regions. Thus, the augmented graphics rendered by the interfaces can support human&#x02013;robot collaboration for object manipulation tasks in shared spaces by forming mixed-reality graphical environments in which the user and robot exchange relevant spatial and task-specific information.</p>
<div class="DottedLine"></div>
<div class="Imageheaders">FIGURE 3</div>
<div class="FigureDesc">
<a href="https://www.frontiersin.org/files/Articles/238204/frobt-04-00020-HTML/image_m/frobt-04-00020-g003.jpg" target="_blank">
<img src="https://www.frontiersin.org/files/Articles/238204/frobt-04-00020-HTML/image_t/frobt-04-00020-g003.gif" id="F3" alt="www.frontiersin.org" /></a>
<p><strong>Figure 3. Screenshot of the mixed-reality environment with the plot of the robot&#x02019;s reachable space enabled</strong>.</p>
</div>
<div class="clear"></div>
<div class="DottedLine"></div>
<p class="mt15 w100pc float_left">In the background of each interface, the mobile application must perform several tasks. First, each video frame is processed using an adaptive thresholding technique to detect the image locations of the centers of the markers affixed to the workspace and objects of interest. Since the twelve workspace markers are coplanar, if any four or more workspace markers have been detected in the video frame then an iterative approach is used to estimate the relative pose of the plane of the workspace through the solution of a perspective <i>n</i>-point problem (<a href="#B26">Oberkampf et al., 1996</a>). To estimate the relative pose of each object of interest, the same approach is applied to the solution of a perspective 4-point problem for each object marker, where the points used are the four corners of the marker. Next, the relative poses are transformed so that the poses of all objects of interest are represented with respect to the fixed coordinate frame established by the workspace markers (see Figure <a href="#F4">4</a>). These transformations allow both the interface to augment the video with virtual elements that enhance the user&#x02019;s situational awareness and provided the pose of the robot is known with respect to the workspace frame, the performance of object manipulation tasks by the robot using the vision-based measurements obtained by the interface.</p>
<div class="DottedLine"></div>
<div class="Imageheaders">FIGURE 4</div>
<div class="FigureDesc">
<a href="https://www.frontiersin.org/files/Articles/238204/frobt-04-00020-HTML/image_m/frobt-04-00020-g004.jpg" target="_blank">
<img src="https://www.frontiersin.org/files/Articles/238204/frobt-04-00020-HTML/image_t/frobt-04-00020-g004.gif" id="F4" alt="www.frontiersin.org" /></a>
<p><strong>Figure 4. Coordinate frames used by both the interface and robot to communicate spatial information</strong>.</p>
</div>
<div class="clear"></div>
<div class="DottedLine"></div>
<p class="mb0 w100pc float_left mt15">To determine which object users would like the robot to manipulate and at what pose they would like the robot to place the object, the interface first captures touch gestures (taps, drags, rotations, and releases) of the users&#x02019; fingers on the screen. Then, the relative transformation to the workspace frame is used to map users&#x02019; touch gestures on the screen to locations and orientations in the plane of the workspace with respect to this reference frame. As users interact with the virtual objects on the screen, the interface uses the mapped poses to display these objects underneath the users&#x02019; fingers, creating the sensation that they are moving the block along the surface of the table. Once users have completed their interactions with the virtual blocks and are satisfied with their interactions, a press of a button communicates the associated spatial commands to the robot that enable it to manipulate the corresponding physical object to the appropriate desired pose. Open source libraries are used to perform these tasks, such as the Open Source Computer Vision (OpenCV) library to process video frames, Open Source Graphics Library for Embedded Systems (OpenGL ES) to render augmented graphics, and the CocoaAsyncSocket library to communicate with the robot over Wi-Fi using the TCP/IP protocol.</p>
<h3>3.1. Conventional Egocentric Interface</h3>
<p class="mb0">With the first interface, users interact with the robot while being provided visual feedback from the perspective of one of the robot&#x02019;s cameras, which is mounted on a pan and tilt system. The limited field of view of this camera is only capable of capturing one of the three sections of interest in the workspace at any given time (see Figure <a href="#F5">5</a>). Thus, users are forced to pan and tilt the camera between these three sections if objects are to be moved from one section of the workspace to another. To give users the most natural control of the pan and tilt of the robot&#x02019;s camera system, while reserving touchscreen gestures for interactions with virtual elements in the mixed reality environment, the interface uses the device&#x02019;s accelerometer and gyroscope to estimate the device&#x02019;s attitude. These estimates, which are obtained at a rate of 30 Hz, are represented as Euler angles with respect to a fixed reference frame whose <i>Z</i> axis is vertical along the downward-facing gravity vector and whose <i>X</i> axis points along the centroidal axis in the longitudinal direction of the tablet from when the interaction was initiated. Readings from the magnetometer are used to correct the direction of the <i>X</i> axis to maintain long-term accuracy. Figure <a href="#F6">6</a> shows the yaw rotation <i>&#x003B8;</i> that transforms the <i>XYZ</i> frame to an intermediate frame <i>x</i>&#x02032;<i>y</i>&#x02032;<i>z</i>&#x02032; and the roll rotation <i>&#x003D5;</i> that transforms the intermediate frame <i>x</i>&#x02032;<i>y</i>&#x02032;<i>z</i>&#x02032; to the frame <i>xyz</i> attached to the device. Since the yaw and roll of the device are directly analogous to the pan and tilt, respectively, of the robot&#x02019;s camera system, a one-to-one mapping is used to command the attitude of the robot&#x02019;s camera system from the estimated attitude of the device. However, the camera system can pan only 270&#x000B0; and tilt only &#x000B1;45&#x000B0;. Due to these limitations in the mobility of robot&#x02019;s camera system and the limited field of view of its cameras, there are locations in the workspace that are unobservable using this interface (i.e., close to the robot as well as behind the robot).</p>
<div class="DottedLine"></div>
<div class="Imageheaders">FIGURE 5</div>
<div class="FigureDesc">
<a href="https://www.frontiersin.org/files/Articles/238204/frobt-04-00020-HTML/image_m/frobt-04-00020-g005.jpg" target="_blank">
<img src="https://www.frontiersin.org/files/Articles/238204/frobt-04-00020-HTML/image_t/frobt-04-00020-g005.gif" id="F5" alt="www.frontiersin.org" /></a>
<p><strong>Figure 5. Egocentric views of the workspace as captured from the robot-mounted camera</strong>.</p>
</div>
<div class="clear"></div>
<div class="DottedLine mb15"></div>
<div class="Imageheaders">FIGURE 6</div>
<div class="FigureDesc">
<a href="https://www.frontiersin.org/files/Articles/238204/frobt-04-00020-HTML/image_m/frobt-04-00020-g006.jpg" target="_blank">
<img src="https://www.frontiersin.org/files/Articles/238204/frobt-04-00020-HTML/image_t/frobt-04-00020-g006.gif" id="F6" alt="www.frontiersin.org" /></a>
<p><strong>Figure 6. Rotations considered between coordinate frames in estimating the roll and yaw of the device</strong>.</p>
</div>
<div class="clear"></div>
<div class="DottedLine"></div>
<h3>3.2. Conventional Exocentric Interface</h3>
<p class="mb0">To improve upon the limitations encountered with the first interface in observing the workspace, a second interface provides users with an expanded and more natural view from a camera mounted to the ceiling above the workspace (see Figure <a href="#F7">7</a>). Since the camera is fixed, no device motion is captured by the interface. Users simply interact with the mixed-reality environment generated from an overhead perspective of the workspace. However, there remain locations in the workspace that cannot be observed from the perspective presented by this interface (i.e., underneath the robot&#x02019;s arms). Due to the inability to adjust the perspective provided by this interface, users still cannot interact with objects everywhere in the robot&#x02019;s reachable space without the installation of additional cameras.</p>
<div class="DottedLine"></div>
<div class="Imageheaders">FIGURE 7</div>
<div class="FigureDesc">
<a href="https://www.frontiersin.org/files/Articles/238204/frobt-04-00020-HTML/image_m/frobt-04-00020-g007.jpg" target="_blank">
<img src="https://www.frontiersin.org/files/Articles/238204/frobt-04-00020-HTML/image_t/frobt-04-00020-g007.gif" id="F7" alt="www.frontiersin.org" /></a>
<p><strong>Figure 7. Exocentric view of the workspace as captured from an overhead camera</strong>.</p>
</div>
<div class="clear"></div>
<div class="DottedLine"></div>
<h3>3.3. Proposed Mobile Mixed-Reality Interface</h3>
<p class="mb0">With the third interface, users hold the tablet such that its rear-facing camera is pointed at the robot and its workspace from an arbitrary perspective. In this interface design, the tablet&#x02019;s camera captures the video used to render the mixed-reality environment on the screen for interacting with the robot (see Figure <a href="#F8">8</a>). The mobility afforded with this interface approach offers a few distinct advantages over the other two approaches, particularly the ability to access every location in the robot&#x02019;s reachable space without the need for installed sensing or computation beyond the mobile device. These features allow for more seamless and economic implementation of instrumented human&#x02013;robot interactions in a wider range of applications.</p>
<div class="DottedLine"></div>
<div class="Imageheaders">FIGURE 8</div>
<div class="FigureDesc">
<a href="https://www.frontiersin.org/files/Articles/238204/frobt-04-00020-HTML/image_m/frobt-04-00020-g008.jpg" target="_blank">
<img src="https://www.frontiersin.org/files/Articles/238204/frobt-04-00020-HTML/image_t/frobt-04-00020-g008.gif" id="F8" alt="www.frontiersin.org" /></a>
<p><strong>Figure 8. Mobile view of the workspace as captured from the tablet&#x02019;s rear-facing camera</strong>.</p>
</div>
<div class="clear"></div>
<div class="DottedLine"></div>
<a id="h5"></a><h2>4. Evaluation</h2>
<p class="mb15">To assess the user experience and performance in interacting with the humanoid robot using the developed interfaces, an experimental study was conducted with participants, who were each asked to command the robot to complete an object manipulation task using one of the interfaces. The objective of the task was to pick up each of the three blocks in the workspace and to place it in the container of matching color in as little time as possible. See <a href="http://engineering.nyu.edu/mechatronics/videos/mmrmanipulation.html">http://engineering.nyu.edu/mechatronics/videos/mmrmanipulation.html</a> for a video illustrating user interaction for task completion with each of the three interfaces. The interface assignment for each participant was generated randomly to prevent any potentially biased data. The proposed mobile mixed-reality interface has a distinct advantage over the other two interfaces since, if one of the blocks were placed directly underneath either of the robot&#x02019;s arms, that block will be out of the field of vision of both the egocentric and exocentric interfaces. This would cause participants to lose visible and interactive access to the block from either of these interfaces. This advantage of the mobile mixed-reality interface exists in general when only a single camera is mounted to either the environment or the robot, since the robot and obstacles in the environment will always occlude a portion of the workspace. Thus, to level the playing field and ensure consistent and unbiased experimental data, the three blocks started in locations that were visible with each of the three interfaces.</p>
<p class="mb15">First, each participant completed a preliminary assessment, which was used to gage the participant&#x02019;s familiarity with mobile devices, AR, and robots. Then, participants were given a one-minute introduction to the task to be performed and the interface to be used to perform the task. Participants were instructed about the objective of the task, i.e., to place each block in the container of the same color. Participants using the mobile interface were instructed to keep as many workspace markers on the screen as possible to achieve the most accurate performance. Each participant was given only one chance to place each block.</p>
<p class="mb15">Prior to performing the task, with the containers removed from the workspace, each participant was given one pretrial to practice commanding the robot to pick up and place the green block to an arbitrary pose in the workspace. For half of the participants (10 in each group of 20 participants), the augmented plot of the robot&#x02019;s reachable space was enabled, and for the other half the plot was disabled. Due to the lengths and configurations of the robot&#x02019;s arms, and the grasping approach implemented in the study, approximately 80.1% of the workspace is unreachable, 18.7% can be reached by one of the robot&#x02019;s arms, and 1.2% can be reached by both arms. Thus, the pretrials were conducted to evaluate the extent to which the augmented plot improves participants&#x02019; situational awareness such that they can successfully complete an object manipulation to a reachable location in the workspace.</p>
<p class="mb15">As participants performed the task, they stood facing the robot from across the semicircular section of the workspace. With the third interface, participants were encouraged to move around as needed to see blocks that might be hidden behind the robot. To assess the participants&#x02019; performance of the task using each of the interfaces, the time elapsed and the success of each trial were manually recorded by the evaluator observing the trial. Since participants were asked to pick up and place three blocks, their success was graded as the percentage (from 0 to 100%) of these six operations that they completed successfully.</p>
<p class="mb0">After participants performed the task, they were asked to respond to an evaluation that assessed their experiences in two parts. In the first part of the evaluation, the participants were asked to indicate their level of agreement with nine positive and negative statements on a 5-point scale. In the second part of the evaluation, the NASA Task Load indeX (NASA-TLX) was used to assess the workload associated with using each interface (<a href="#B10">Hart, 2006</a>). This index, which is calculated on a 0&#x02013;100 scale, indicates the overall demand of the task on the participant and is composed of six categories: mental workload, physical workload, temporal workload, performance, effort, and frustration. To obtain the NASA-TLX, participants complete a workload evaluation sheet in which, for each of the six categories, they mark an &#x0201C;X&#x0201D; on the 0&#x02013;100 scale that is separated into 20 equally spaced segments of five points. In the Raw TLX (RTLX) variation of the assessment, the ratings reported for each category are uniformly weighed and used to calculate an average overall workload for the task. To aid them in carefully responding to the workload evaluation, participants were provided with a guide that described each of the categories in detail. Finally, at the end of the evaluation, participants were provided a blank sheet of paper where they were encouraged to leave comments and feedback about their experience while performing the task.</p>
<a id="h6"></a><h2>5. Evaluation Results and Discussion</h2>
<p class="mb0">To evaluate aspects of the performance and user experience associated with interacting with the humanoid robot using each of the three user interfaces, the study was conducted with 60 undergraduate engineering student participants. All 60 participants owned a smartphone and 61.7% owned a tablet. Since participants were students in engineering, roughly half (50.6%) had interacted with, worked on, or built a robot before, while the rest (49.6%) had neither seen nor interacted with a robot up close prior to their participation in the study. Due to recent releases of popular games and utilities with augmented reality, 38.1% of the participants reported having at least one recent mobile augmented reality experience. Figure <a href="#F9">9</a> summarizes participants&#x02019; self-reported levels of familiarity with each of the emerging technologies relevant to the study.</p>
<div class="DottedLine"></div>
<div class="Imageheaders">FIGURE 9</div>
<div class="FigureDesc">
<a href="https://www.frontiersin.org/files/Articles/238204/frobt-04-00020-HTML/image_m/frobt-04-00020-g009.jpg" target="_blank">
<img src="https://www.frontiersin.org/files/Articles/238204/frobt-04-00020-HTML/image_t/frobt-04-00020-g009.gif" id="F9" alt="www.frontiersin.org" /></a>
<p><strong>Figure 9. The average levels of familiarity with various emerging technologies reported by participants</strong>.</p>
</div>
<div class="clear"></div>
<div class="DottedLine"></div>
<h3>5.1. Performance</h3>
<p class="mb0">As participants interacted with the robot using each of the interfaces, their performance of the task was evaluated by recording the number of successful pickups and placements of each block on the table and the amount of time taken to complete the task.</p>
<h4>5.1.1. Success Rate</h4>
<p class="mb0">Of the 60 participants in the study, 60% performed the pretrial successfully. Of these 36 participants, 72.2% had the augmented plot of the robot&#x02019;s reachable space enabled. Of the 24 that failed the pretrial, 83.3% did not have the augmented plot enabled. Table <a href="#T1">1</a> shows the number of participants who had the augmented plot enabled and passed the pretrial, those who had the augmented plot enabled but failed the pretrial, those who did not have the plot enabled but passed the pretrial, and those who did not have the plot enabled and failed the pretrial. The majority of participants who were provided the augmented plot succeeded in accomplishing the test (86.67%) and those who were not provided the augmented plot failed in accomplishing the test (66.67%). To determine whether access to the augmented plot had a significant effect on participants&#x02019; success in the pretrial, the <i>N</i>-1 Two-Proportion Test proposed by <a href="#B2">Campbell (2007)</a> was used and resulted in rejection of the null hypothesis that participants perform equally well with and without the augmented plot (<i>z</i> &#x0003D; 4.181, <i>p</i> &#x0003C; 0.0001). These results indicate the significant benefit that the augmented plot has in raising participants&#x02019; awareness of the robot&#x02019;s spatial limitations. Note that, although usability issues prevented a small group of participants from passing the pretrials with the augmented plot enabled, 33.33% of the participants were able to accomplish the pretrial successfully without access to the plot. Since 18.7% of the workspace is reachable, the participants without the augmented plot who guessed a reachable location is nearly double of the amount that would be expected as a chance occurrence. This is due to several participants&#x02019; preferences to command the blocks to locations directly in front of the robot, where most of the reachable space is concentrated. Note that no statistically significant differences were found between the mean performances of the pretrial achieved with each interface.</p>
<div class="DottedLine"></div>
<div class="Imageheaders">TABLE 1</div>
<div class="FigureDesc">
<a href="https://www.frontiersin.org/files/Articles/238204/frobt-04-00020-HTML/image_m/frobt-04-00020-t001.jpg" target="_blank">
<img src="https://www.frontiersin.org/files/Articles/238204/frobt-04-00020-HTML/image_t/frobt-04-00020-t001.gif" id="T1" alt="www.frontiersin.org" /></a>
<p><strong>Table 1. The performance of participants in the pretrial, with and without access to the augmented plot</strong>.</p>
</div>
<div class="clear"></div>
<div class="DottedLine"></div>
<p class="mt15 w100pc float_left">Overall, participants performed the object manipulation task very well with each of the interfaces. Of each group of 20 participants, 16, 18, and 17 participants succeeded in picking and placing at least 2 of the 3 blocks using the egocentric, exocentric, and mobile mixed-reality interfaces, respectively. Participants performed an average of 86.7%, 84.2%, and 91.7% of their interactions successfully with the red, green, and blue blocks, respectively. Figure <a href="#F10">10</a> shows the average percentage of successful pick and place operations performed by participants using each interface, both for each of the blocks and overall during a trial. Although Figure <a href="#F10">10</a> shows that, on average, participants were more successful with the green block and performed better using exocentric interface than the other two, no statistically significant differences were found, neither between interactions with the different blocks nor between the different interfaces. In other words, participants perform the task just as well with the mobile mixed-reality interface (which communicates directly with the robot without the need for robot-mounted or environmentally mounted sensors) as with the egocentric or exocentric interfaces (which require cameras and their associated processing to be installed in the environment or onto the robot).</p>
<div class="DottedLine"></div>
<div class="Imageheaders">FIGURE 10</div>
<div class="FigureDesc">
<a href="https://www.frontiersin.org/files/Articles/238204/frobt-04-00020-HTML/image_m/frobt-04-00020-g010.jpg" target="_blank">
<img src="https://www.frontiersin.org/files/Articles/238204/frobt-04-00020-HTML/image_t/frobt-04-00020-g010.gif" id="F10" alt="www.frontiersin.org" /></a>
<p><strong>Figure 10. Rates of successful pick and place operations accomplished using each interface</strong>.</p>
</div>
<div class="clear"></div>
<div class="DottedLine"></div>
<h4>5.1.2. Time</h4>
<p class="mb0">Although participants completed the task at the same rate using each of the interfaces, the amount of time taken to complete the task differed depending on which interface was used. Figure <a href="#F11">11</a> presents the sample medians of the times elapsed while performing the task with each of the interfaces (169, 132.5, and 139 s with the egocentric, exocentric, and mobile mixed-reality interfaces, respectively), along with their 95% confidence intervals, calculated according to <a href="#B28">Sauro and Lewis (2012)</a>. With each interface, the same amount of the trial time is spent waiting for the robot to complete its operations on the blocks. Thus, differences in total trial times are largely due to differences in interaction time of the participants, which can arise from variations between participants or between the efforts required to perform the task using different interfaces. The results of independent two-tailed <i>t</i>-tests indicate no statistically significant differences between the egocentric and mobile interfaces or between the exocentric and mobile interfaces. However, a statistically significant difference is found between the egocentric and exocentric interfaces (<i>t</i>(38) &#x0003D; 7.0636, <i>p</i> &#x0003D; 2.0174 &#x000D7; 10<sup>&#x02212;8</sup> &#x0003C; 0.05). This difference in time can be explained by the distinct amounts of mental and physical efforts demanded by each interface. Specifically, participants using the exocentric interface do not have to change the visual perspective provided by the interface. Thus, participants only need to become comfortable with the visual perspective once and can spend most of the interaction time manipulating the augmented graphics. Meanwhile, using the egocentric interface required participants to physically turn their bodies to adjust the perspective of the robot-mounted camera. Use of the mobile interface requires participants to physically move around the environment to adjust the interface&#x02019;s visual perspective. However, since participants with mobile interface choose very different vantage points from which to observe the workspace and take largely varying amounts of time to get settled at these vantage points, there is much larger variation in the time they take to complete the task than participants with the other two interfaces. In fact, the standard deviation of the times taken with the mobile interface (42.1 s) is more than twice the amount with the egocentric interface (16.8 s) and with the exocentric interface (19.9 s). Thus, to calculate the degrees of freedom for the <i>t</i>-tests involving the mobile interface, the Welch&#x02013;Satterthwaite procedure is used to account for these large differences in variances (<a href="#B28">Sauro and Lewis, 2012</a>). Although the egocentric interface required less physical motion than the mobile interface, it will be shown that participants found the egocentric perspective unnatural and uncomfortable. Thus, participants using the egocentric interface may have spent a large amount of interaction time readjusting to changes in visual perspective.</p>
<div class="DottedLine"></div>
<div class="Imageheaders">FIGURE 11</div>
<div class="FigureDesc">
<a href="https://www.frontiersin.org/files/Articles/238204/frobt-04-00020-HTML/image_m/frobt-04-00020-g011.jpg" target="_blank">
<img src="https://www.frontiersin.org/files/Articles/238204/frobt-04-00020-HTML/image_t/frobt-04-00020-g011.gif" id="F11" alt="www.frontiersin.org" /></a>
<p><strong>Figure 11. Average amount of time spent performing the object manipulation task with each interface</strong>.</p>
</div>
<div class="clear"></div>
<div class="DottedLine"></div>
<h3>5.2. User Experience</h3>
<p class="mb0">In addition to the performance achieved, participants&#x02019; perceptions of their experience form a fundamental part of evaluating the mixed-reality interfaces. The user experience associated with each interface was assessed using three mechanisms: a usability questionnaire, a NASA RTLX self-assessment, and by reviewing participant comments and feedback.</p>
<h4>5.2.1. Usability Questionnaire Results</h4>
<p class="mb15">To assess aspects of participants&#x02019; user experience, a questionnaire inspired by the Post-Study System Usability Questionnaire (<a href="#B17">Lewis, 2002</a>) was designed and administered to participants. This questionnaire asked participants to indicate their level of agreement with the following nine positive and negative statements on a 5-point scale (1: strong disagreement and 5: strong agreement).</p>
<p class="mb0" style="margin-left:1.5em; text-indent:-1.5em;">a. It was difficult to interact with the virtual elements on the screen.</p>
<p class="mb0" style="margin-left:1.5em; text-indent:-1.5em;">b. The virtual graphics on the screen were useful visual aids.</p>
<p class="mb0" style="margin-left:1.5em; text-indent:-1.5em;">c. Overall, the application made it easy and fun to interact with the robot.</p>
<p class="mb0" style="margin-left:1.5em; text-indent:-1.5em;">d. I required assistance to interact with the robot.</p>
<p class="mb0" style="margin-left:1.5em; text-indent:-1.5em;">e. It took a long time for me to become comfortable using the application.</p>
<p class="mb0" style="margin-left:1.5em; text-indent:-1.5em;">f. It was easy to place and orient blocks on the table using this application.</p>
<p class="mb0" style="margin-left:1.5em; text-indent:-1.5em;">g. Overall, I felt that I was able to use the application to accurately communicate my intentions to the robot.</p>
<p class="mb0" style="margin-left:1.5em; text-indent:-1.5em;">h. Overall, I would recommend this application to people who work with robots at home or work.</p>
<p class="mb0" style="margin-left:1.5em; text-indent:-1.5em;">i. I would like to see more applications like this for people who may one day have robots at home or work.</p>
<p class="mb0">Figure <a href="#F12">12</a> shows the participant responses for each statement in the questionnaire. These responses look promising, indicating relatively low amounts of perceived difficulty and relatively moderate to high amounts of satisfaction and perceived performance with the interfaces. By conducting independent two-tailed <i>t</i>-tests on the response data, no statistically significant differences were found between the responses given by participants using the exocentric interface and those using the mobile interface. However, several statistically significant differences were found between the responses given by participants using the egocentric interface and those using the exocentric and mobile interfaces. For example, although many participants either disagreed or strongly disagreed that any of the interfaces made it difficult to interact with the virtual elements on the screen, participants using the egocentric interface found that it was more difficult than those using exocentric interface (<i>t</i>(38) &#x0003D; 4.7721, <i>p</i> &#x0003D; 2.6985 &#x000D7; 10<sup>&#x02212;5</sup> &#x0003C; 0.05). Moreover, participants using the egocentric interface found that it was not as easy to place and orient blocks on the table as those who used either the exocentric interface (<i>t</i>(38) &#x0003D; 3.3040, <i>p</i> &#x0003D; 0.0021 &#x0003C; 0.05) or mobile interface (<i>t</i>(38) &#x0003D; 2.3576, <i>p</i> &#x0003D; 0.0236 &#x0003C; 0.05). Overall, participants using the egocentric interface consistently agreed less with the statement that: (1) it was easy and fun to interact with the robot than those who used the exocentric interface (<i>t</i>(38) &#x0003D; 3.2302, <i>p</i> &#x0003D; 0.0026 &#x0003C; 0.05) or mobile interface (<i>t</i>(38) &#x0003D; 2.0353, <i>p</i> &#x0003D; 0.0488 &#x0003C; 0.05) interfaces and (2) they were able to use the application to accurately communicate their intentions to the robot than those who used the exocentric interface (<i>t</i>(38) &#x0003D; 3.7173, <i>p</i> &#x0003D; 6.4693 &#x000D7; 10<sup>&#x02212;4</sup> &#x0003C; 0.05) or mobile interface (<i>t</i>(38) &#x0003D; 2.1475, <i>p</i> &#x0003D; 0.0382 &#x0003C; 0.05). These differences in perceived difficulty and ability could be due to the fact that using the egocentric interface requires participants to first turn the device to find the object of interest, tap on the object to select it, then turn again to find a view containing the goal location for the object and finally tap and drag on the goal location, whereas the exocentric interface contains both the start and goal locations of each object in the view at all times and does not require alternating sequences of device movements and touchscreen gestures to interact with the robot. However, since the mobile interface sometimes may require some movement to achieve a convenient perspective of the workspace, participants find its level of difficulty somewhere between that of the two conventional interface designs on average.</p>
<div class="DottedLine"></div>
<div class="Imageheaders">FIGURE 12</div>
<div class="FigureDesc">
<a href="https://www.frontiersin.org/files/Articles/238204/frobt-04-00020-HTML/image_m/frobt-04-00020-g012.jpg" target="_blank">
<img src="https://www.frontiersin.org/files/Articles/238204/frobt-04-00020-HTML/image_t/frobt-04-00020-g012.gif" id="F12" alt="www.frontiersin.org" /></a>
<p><strong>Figure 12. Participants&#x02019; levels of agreement with each statement in the user experience questionnaire</strong>.</p>
</div>
<div class="clear"></div>
<div class="DottedLine"></div>
<p class="mb0 w100pc float_left mt15">The difficulty and discomfort associated with needing to move the device to find objects and goal locations in the perspective of the robot explain why the egocentric interface required more time to complete the task than the other two interfaces. This argument is supported by the finding that participants using the egocentric interface responded that they required more time to become comfortable using the interface than participants using the exocentric interface (<i>t</i>(38) &#x0003D; 2.2736, <i>p</i> &#x0003D; 0.0287 &#x0003C; 0.05) or mobile interface (<i>t</i>(38) &#x0003D; 3.4183, <i>p</i> &#x0003D; 0.0015 &#x0003C; 0.05). Moreover, participants using the egocentric interface responded that they required more assistance from the laboratory assistant in order to the use the interface than participants using the exocentric interface (<i>t</i>(38) &#x0003D; 2.1954, <i>p</i> &#x0003D; 0.0343 &#x0003C; 0.05).</p>
<h4>5.2.2. Workload</h4>
<p class="mb0">Further insight into the user experience associated with performing the task with each interface is obtained by reviewing the responses to the workload evaluation. Using the RTLX approach, the ratings reported for each category are used to compute the average overall workload index. Figure <a href="#F13">13</a> shows the mean values of the workload reported by the participants for each category, as well as the mean values of overall workload computed from these reported values. These results are promising since they show that participants using each interface reported a relatively low workload in each of the categories. As was the case with the user experience results, no statistically significant differences were found between the exocentric and mobile interfaces. However, statistically significant differences were found between the overall index computed for the egocentric interface and both the exocentric interface (<i>t</i>(38) &#x0003D; 3.0592, <i>p</i> &#x0003D; 0.0041 &#x0003C; 0.05) and the mobile interface (<i>t</i>(38) &#x0003D; 2.2041, <i>p</i> &#x0003D; 0.0336 &#x0003C; 0.05). These differences are due to several statistically significant differences that were found between participants&#x02019; perceived mental workload, physical workload, and effort with the egocentric interface and the workload amounts perceived by participants using the other two interfaces. Specifically, participants using the egocentric interface reported experiencing both significantly higher mental and physical workload than the participants using either the exocentric interface (mental: <i>t</i>(38) &#x0003D; 2.1804, <i>p</i> &#x0003D; 0.0355 &#x0003C; 0.05, physical: <i>t</i>(38) &#x0003D; 5.8473, <i>p</i> &#x0003D; 0.0343 &#x0003C; 0.05) or mobile interface (mental: <i>t</i>(38) &#x0003D; 2.1941, <i>p</i> &#x0003D; 9.2227 &#x000D7; 10<sup>&#x02212;7</sup> &#x0003C; 0.05, physical: <i>t</i>(38) &#x0003D; 2.3158, <i>p</i> &#x0003D; 0.0261 &#x0003C; 0.05). These results are consistent with comments made earlier in the paper regarding the disadvantages of using the egocentric perspective. Note that although participants using the mobile interface, on average, report experiencing more than twice the amount of physical workload than those using the exocentric interface, a large variance in the physical workload perceived by these participants prevents this difference from being statistically significant. This large variance could be due to the fact that there were several options for how participants needed to move to interact with a block that was hidden behind the robot, resulting in different amounts of movement for different participants. Moreover, varying amounts of comfort and engagement with the mobile interface may have influenced participants&#x02019; perceptions of the physical demands of the task. Despite significant differences in recorded trial times, significant differences were not found between the temporal demand experienced with different interfaces. Furthermore, despite significant differences between the level of difficulty experienced by participants, significant differences were not found in the workload evaluation between the frustration experienced by participants using different interfaces. However, a significant difference was found in the reported effort required to accomplish the task between participants using the egocentric interface and those using the exocentric interface. These results are consistent with the significant differences found in mental and physical workload.</p>
<div class="DottedLine"></div>
<div class="Imageheaders">FIGURE 13</div>
<div class="FigureDesc">
<a href="https://www.frontiersin.org/files/Articles/238204/frobt-04-00020-HTML/image_m/frobt-04-00020-g013.jpg" target="_blank">
<img src="https://www.frontiersin.org/files/Articles/238204/frobt-04-00020-HTML/image_t/frobt-04-00020-g013.gif" id="F13" alt="www.frontiersin.org" /></a>
<p><strong>Figure 13. Workload reported by participants for each category, as well as the computed overall index</strong>.</p>
</div>
<div class="clear"></div>
<div class="DottedLine"></div>
<h4>5.2.3. Participant Comments</h4>
<p class="mb15">Further insights regarding the differences in user experiences and perceived workload between the interfaces can be obtained by reviewing the participants&#x02019; comments and feedback. Of the 60 participants in the study, 44 provided descriptive comments and suggestions for future improvement to the design of the interfaces. Comments included praises, positively biased statements that do not offer recommendations for improvement, such as</p>
<p class="mb15" style="margin-left:1.5em; margin-right:1.5em;">The interface definitely made it fun and easy to interact with the robot.&#x02014;Participant 22 who used the proposed mobile mixed-reality interface.</p>
<p class="mb15">There were also a few criticisms, characterized as negatively biased statements that do not offer recommendations for improvement, such as</p>
<p class="mb15" style="margin-left:1.5em; margin-right:1.5em;">The video had significant lag.&#x02014;Participant 52 who used the conventional exocentric interface.</p>
<p class="mb15">However, most participants left useful suggestions for improvement that were neither positively or negatively biased, such as</p>
<p class="mb15" style="margin-left:1.5em; margin-right:1.5em;">Perhaps make the augmented grid lines less visible so they are more user-friendly.&#x02014;Participant 18 who used the conventional egocentric interface.</p>
<p class="mb0">Figure <a href="#F14">14</a> presents the percentages of each type of comment left by participants using each interface in the study. These results show that a majority of the participants were affected enough by their participation in the study to leave meaningful comments for developers and that a sizable portion of participants were satisfied enough to praise their experience in writing. Praises were taken as validation of our arguments for integrating mobile hardware and software in the development of mixed-reality interfaces for interacting with robots, such as the ones proposed in this paper. Many of the praises touched on the satisfaction associated with being able to successfully control the actions of the humanoid robot. Participants were also impressed with the ability to naturally interact with the augmented blocks in the mixed-reality environment and the intimate connection observed between their actions with the augmented blocks and the resulting operations performed by the robot on the physical blocks. Criticisms served as a reminder that this class of mobile mixed-reality human&#x02013;robot interfaces is still in its infancy, and much work remains to be done to improve the visual and interactive aspects of the interfaces. These criticisms included the observation that a small amount of delay was present in the video provided by the egocentric and exocentric interfaces. Moreover, although the overhead camera was placed at the minimal distance needed to capture the entire workspace, participants felt the perspective used by the exocentric interface was too far above the table, making the blocks appear too small and difficult to interact with. Furthermore, several participants who used the egocentric interface found it uncomfortable to see themselves performing the task from the perspective of the robot. Criticisms of the mobile interface were not related to the perspective provided by the camera or any video delay, but instead were focused on usability issues associated with the visual and interactive aspects of the interface. Suggestions included alternative ways that the augmented grid and blocks ought to be displayed in the environment (e.g., changing the appearance and lowering the transparency of the visuals, providing augmented reality containers, etc.), as well as alternative ways in which users ought to interact with these augmented elements (e.g., allowing virtual elements to &#x0201C;snap&#x0201D; to locations in the environment, allowing users to perform pinch gestures on the screen to zoom in and out on the view, etc.). These suggestions will be considered in the development and testing of future prototypes before the next user study with the interfaces is conducted.</p>
<div class="DottedLine"></div>
<div class="Imageheaders">FIGURE 14</div>
<div class="FigureDesc">
<a href="https://www.frontiersin.org/files/Articles/238204/frobt-04-00020-HTML/image_m/frobt-04-00020-g014.jpg" target="_blank">
<img src="https://www.frontiersin.org/files/Articles/238204/frobt-04-00020-HTML/image_t/frobt-04-00020-g014.gif" id="F14" alt="www.frontiersin.org" /></a>
<p><strong>Figure 14. Percentage of each type of comment left by participants for each interface used in the study</strong>.</p>
</div>
<div class="clear"></div>
<div class="DottedLine"></div>
<a id="h7"></a><h2>6. Conclusion</h2>
<p class="mb15">This paper proposed a novel mobile mixed-reality interface approach to interact with robots in shared spaces using the mobile devices that users already own and are familiar with. The proposed approach appropriately leverages the capabilities of mobile devices to render mixed-reality environments from views provided by their rear-facing cameras. This allows for the development of interfaces with more intuitive visual perspectives, reduced latency, improved responsiveness, and expanded perceptual range in comparison to implementing conventional egocentric and exocentric interfaces on mobile devices.</p>
<p class="mb15">To evaluate aspects of the performance and user experience associated with these interfaces, a user study was conducted in which participants interacted with a humanoid robot to perform object manipulation on a tabletop. The study revealed the following benefits of the proposed approach. First, by integrating touchscreen interaction with augmented graphics, the interfaces allowed users to naturally command the robot to manipulate physical objects. Second, the interactions enabled by the approach were as successful and yielded comparable or better user experiences as the conventional interface approaches. Third, the mobility associated with the approach provided the sensation of directly interacting with objects in the robot&#x02019;s workspace through a visually engaging mixed-reality window. Fourth, by allowing users to move about while pointing their device from different perspectives, the approach resolved usability issues commonly exhibited by conventional interfaces, such as the keyhole effect and occlusion. The practical significance of the proposed approach is evidenced from the following. First, the approach provides intuitive and natural interactive mixed-reality interaction with robots without the need for large operator stations or sophisticated vision systems mounted in the environment or on the robot. Second, the approach can be generalized to disparate robots and workspaces, provided that an accurate kinematic model of the robot and an accurate geometric model of the visual features in the environment are known. Third, the approach enables robotic platforms to be operated outside the traditional laboratory environment, which may preclude the installation of vision sensors. Fourth, the approach can significantly reduce the cost and complexity of implementing human&#x02013;robot interaction systems. Finally, the proposed approach can be adapted to a broad range of applications, such as assembly, machining, packaging, and handling tasks commonly encountered in manufacturing and construction.</p>
<p class="mb0">The following are the specific conditions under which the proposed approach works well. First, the approach addresses applications in which the user and robot occupy a shared space. Second, the approach requires <i>n</i> &#x02265; 4 visual markers in the workspace of the robot that are coplanar and detectable by the mobile device&#x02019;s camera from various perspectives. Third, the approach is deemed feasible with mobile devices with comparable or superior capability to an iPad Pro <i>vis-&#x000E0;-vis</i> its sensing and processing capabilities. Despite its benefits, the proposed approach is limited as revealed by the aforementioned specific conditions for it to function well. For example, instead of being limited to shared spaces, conventional approaches can be implemented in remote operation scenarios also. However, the list of applications in which people interact with robots in shared spaces is steadily growing (e.g., in education, medicine, recreation, and domestic applications). Next, as the robot and objects in the workspace obstruct the view of one or more visual markers from the mobile device camera, additional visual markers must be introduced into the workspace (e.g., twelve visual markers were affixed to the tabletop in this study). Thus, to relieve constraints on the robot&#x02019;s workspace, future work will explore alternative methods of plane estimation by exploiting (<i>i</i>) the visual features on the robot rather than the workspace, (<i>ii</i>) the concept of vanishing points, and (<i>iii</i>) the inertial measurement unit of mobile devices. Finally, the proposed approach has only been tested with the robot performing relatively simple pick-and-place tasks on rigid blocks that have simple shapes and are affixed with visual markers. However, a variety of vision techniques exist for detecting and estimating the pose of objects without the use of visual markers. In an effort to generalize the proposed approach to practical applications of robotic manipulation, future efforts will integrate these techniques to the treatment of irregular-shaped or deformable objects. Furthermore, complex tasks with obstacles will be addressed by the fusion of sensor data captured by the mobile device and by the robot to construct a map of the robot&#x02019;s configuration space to be used for path planning.</p>
<a id="h8"></a><h2>Author Contributions</h2>
<p class="mb0">JF is the main contributor, both in the development and research with the mobile mixed-reality interfaces as well as with the humanoid robot. MM was involved in the design and construction of the robot and in the programming of the robotic kinematics and path planning algorithms. VK supervised all aspects of research, including checking the accuracy of results and helping with the writing of the paper.</p>
<a id="h9"></a><h2>Conflict of Interest Statement</h2>
<p class="mb0">The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
<a id="h10"></a><h2>Funding</h2>
<p class="mb0">This work was supported in part by the National Science Foundation awards RET Site EEC-1132482 and EEC-1542286, DRK-12 DRL: 1417769, ITEST DRL: 1614085, and GK-12 Fellows DGE: 0741714, and NY Space Grant Consortium grant 76156-10488.</p>
<a id="h11"></a><h2>References</h2>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B1"></a>Birkenkampf, P., Leidner, D., and Borst, C. (2014). &#x0201C;A knowledge-driven shared autonomy human-robot interface for tablet computers,&#x0201D; in <i>Proc. IEEE-RAS Int. Conf. Humanoid Robots</i> (Madrid: IEEE), 152&#x02013;159.</p>
<p class="ReferencesCopy2"><a href="http://scholar.google.com/scholar_lookup?title=&#x0201C;A+knowledge-driven+shared+autonomy+human-robot+interface+for+tablet+computers,&#x0201D;&#x00026;author=P.+Birkenkampf&#x00026;author=D.+Leidner&#x00026;author=C.+Borst&#x00026;publication_year=2014" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B2"></a>Campbell, I. (2007). Chi-squared and Fisher&#x02013;Irwin tests of two-by-two tables with small sample recommendations. <i>Stat. Med.</i> 26, 3661&#x02013;3675. doi: 10.1002/sim.2832</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1002/sim.2832" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?title=Chi-squared+and+Fisher&#x02013;Irwin+tests+of+two-by-two+tables+with+small+sample+recommendations&#x00026;author=I.+Campbell&#x00026;journal=Stat.+Med.&#x00026;publication_year=2007&#x00026;volume=26&#x00026;pages=3661&#x02013;3675&#x00026;doi=10.1002/sim.2832" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B3"></a>Chen, J. Y., Haas, E. C., and Barnes, M. J. (2007). Human performance issues and user interface design for teleoperated robots. <i>IEEE Trans. Syst. Man. Cybern. C Appl. Rev.</i> 37, 1231&#x02013;1245. doi:10.1109/TSMCC.2007.905819</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1109/TSMCC.2007.905819" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?title=Human+performance+issues+and+user+interface+design+for+teleoperated+robots&#x00026;author=J.+Y.+Chen&#x00026;author=E.+C.+Haas&#x00026;author=M.+J.+Barnes&#x00026;journal=IEEE+Trans.+Syst.+Man.+Cybern.+C+Appl.+Rev.&#x00026;publication_year=2007&#x00026;volume=37&#x00026;pages=1231&#x02013;1245&#x00026;doi=10.1109/TSMCC.2007.905819" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B4"></a>Drury, J. L., Scholtz, J., and Yanco, H. A. (2003). &#x0201C;Awareness in human-robot interactions,&#x0201D; in <i>IEEE Int. Conf. Systems, Man and Cybernetics</i>, Vol. 1 (Washington, DC: IEEE), 912&#x02013;918.</p>
<p class="ReferencesCopy2"><a href="http://scholar.google.com/scholar_lookup?title=&#x0201C;Awareness+in+human-robot+interactions,&#x0201D;&#x00026;author=J.+L.+Drury&#x00026;author=J.+Scholtz&#x00026;author=H.+A.+Yanco&#x00026;publication_year=2003&#x00026;volume=1" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B5"></a>Frank, J. A., Moorhead, M., and Kapila, V. (2016). &#x0201C;Realizing mixed-reality environments with tablets for intuitive human-robot collaboration for object manipulation tasks,&#x0201D; in <i>Proc. Int. Symp. Robot-Human Interactive Communication</i> (New York, NY: IEEE), 302&#x02013;307.</p>
<p class="ReferencesCopy2"><a href="http://scholar.google.com/scholar_lookup?title=&#x0201C;Realizing+mixed-reality+environments+with+tablets+for+intuitive+human-robot+collaboration+for+object+manipulation+tasks,&#x0201D;&#x00026;author=J.+A.+Frank&#x00026;author=M.+Moorhead&#x00026;author=V.+Kapila&#x00026;publication_year=2016" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B6"></a>Fritzsche, M., Elkmann, N., and Schulenburg, E. (2011). &#x0201C;Tactile sensing: a key technology for safe physical human robot interaction,&#x0201D; in <i>Proc. ACM/IEEE Int. Conf. Human-Robot Interaction</i> (Lausanne: IEEE), 139&#x02013;140.</p>
<p class="ReferencesCopy2"><a href="http://scholar.google.com/scholar_lookup?title=&#x0201C;Tactile+sensing:+a+key+technology+for+safe+physical+human+robot+interaction,&#x0201D;&#x00026;author=M.+Fritzsche&#x00026;author=N.+Elkmann&#x00026;author=E.+Schulenburg&#x00026;publication_year=2011" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B7"></a>Goodrich, M., and Olsen, D. Jr. (2003). &#x0201C;Seven principles of efficient human robot interaction,&#x0201D; in <i>IEEE Int. Conf. Systems, Man, and Cybernetics</i>, Vol. 4 (Washington, DC: IEEE), 3942&#x02013;3948.</p>
<p class="ReferencesCopy2"><a href="http://scholar.google.com/scholar_lookup?title=&#x0201C;Seven+principles+of+efficient+human+robot+interaction,&#x0201D;&#x00026;author=M.+Goodrich&#x00026;author=D.+Olsen&#x00026;publication_year=2003&#x00026;volume=4" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B8"></a>Green, S. A., Chase, J. G., Chen, X., and Billinghurst, M. (2008a). &#x0201C;Evaluating the augmented reality human-robot collaboration system,&#x0201D; in <i>Int. Conf. Mechatronics and Machine Vision in Practice</i> (Auckland: IEEE), 575&#x02013;580.</p>
<p class="ReferencesCopy2"><a href="http://scholar.google.com/scholar_lookup?title=&#x0201C;Evaluating+the+augmented+reality+human-robot+collaboration+system,&#x0201D;&#x00026;author=S.+A.+Green&#x00026;author=J.+G.+Chase&#x00026;author=X.+Chen&#x00026;author=M.+Billinghurst&#x00026;publication_year=2008a" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B9"></a>Green, S., Billinghurst, M., Chen, X., and Chase, J. (2008b). Human-robot collaboration: a literature review and augmented reality approach in design. <i>Int. J. Adv. Rob. Syst.</i> 5, 1&#x02013;18. doi:10.5772/5664</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.5772/5664" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?title=Human-robot+collaboration:+a+literature+review+and+augmented+reality+approach+in+design&#x00026;author=S.+Green&#x00026;author=M.+Billinghurst&#x00026;author=X.+Chen&#x00026;author=J.+Chase&#x00026;journal=Int.+J.+Adv.+Rob.+Syst.&#x00026;publication_year=2008b&#x00026;volume=5&#x00026;pages=1&#x02013;18&#x00026;doi=10.5772/5664" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B10"></a>Hart, S. G. (2006). &#x0201C;Nasa-task load index (NASA-TLX); 20 years later,&#x0201D; in <i>Proc. Human Factors and Ergonomics Society Annual Meeting</i>, Vol. 50 (San Francisco, CA: Sage Publications), 904&#x02013;908.</p>
<p class="ReferencesCopy2"><a href="http://scholar.google.com/scholar_lookup?title=&#x0201C;Nasa-task+load+index+(NASA-TLX);+20+years+later,&#x0201D;&#x00026;author=S.+G.+Hart&#x00026;publication_year=2006&#x00026;volume=50" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B11"></a>Hashimoto, S., Ishida, A., Inami, M., and Igarashi, T. (2011). &#x0201C;Touchme: an augmented reality based remote robot manipulation,&#x0201D; in <i>Proc. Int. Conf. Artificial Reality and Telexistence</i> (Osaka: CiteSeerX).</p>
<p class="ReferencesCopy2"><a href="http://scholar.google.com/scholar_lookup?title=&#x0201C;Touchme:+an+augmented+reality+based+remote+robot+manipulation,&#x0201D;&#x00026;author=S.+Hashimoto&#x00026;author=A.+Ishida&#x00026;author=M.+Inami&#x00026;author=T.+Igarashi&#x00026;publication_year=2011" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B12"></a>Hughes, S. B., and Lewis, M. (2005). Task-driven camera operations for robotic exploration. <i>IEEE Trans. Syst. Man. Cybern. A Syst. Hum.</i> 35, 513&#x02013;522. doi:10.1109/TSMCA.2005.850602</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1109/TSMCA.2005.850602" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?title=Task-driven+camera+operations+for+robotic+exploration&#x00026;author=S.+B.+Hughes&#x00026;author=M.+Lewis&#x00026;journal=IEEE+Trans.+Syst.+Man.+Cybern.+A+Syst.+Hum.&#x00026;publication_year=2005&#x00026;volume=35&#x00026;pages=513&#x02013;522&#x00026;doi=10.1109/TSMCA.2005.850602" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B13"></a>Kao, M.-C., and Li, T.-H. (2010). &#x0201C;Design and implementation of interaction system between humanoid robot and human hand gesture,&#x0201D; in <i>Proc. of SICE Annual Conf</i> (Taipei: IEEE), 1616&#x02013;1621.</p>
<p class="ReferencesCopy2"><a href="http://scholar.google.com/scholar_lookup?title=&#x0201C;Design+and+implementation+of+interaction+system+between+humanoid+robot+and+human+hand+gesture,&#x0201D;&#x00026;author=M.+C.+Kao&#x00026;author=T.+H.+Li&#x00026;publication_year=2010" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B14"></a>Lee, W., Ryu, H., Yang, G., Kim, H., Park, Y., and Bang, S. (2007). Design guidelines for map-based human&#x02013;robot interfaces: a colocated workspace perspective. <i>Int. J. Indus. Ergon.</i> 37, 589&#x02013;604. doi:10.1016/j.ergon.2007.03.004</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1016/j.ergon.2007.03.004" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?title=Design+guidelines+for+map-based+human&#x02013;robot+interfaces:+a+colocated+workspace+perspective&#x00026;author=W.+Lee&#x00026;author=H.+Ryu&#x00026;author=G.+Yang&#x00026;author=H.+Kim&#x00026;author=Y.+Park&#x00026;author=S.+Bang&#x00026;journal=Int.+J.+Indus.+Ergon.&#x00026;publication_year=2007&#x00026;volume=37&#x00026;pages=589&#x02013;604&#x00026;doi=10.1016/j.ergon.2007.03.004" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B15"></a>Lenz, C., Grimm, M., R&#x000F6;der, T., and Knoll, A. (2012). &#x0201C;Fusing multiple kinects to survey shared human-robot-workspaces,&#x0201D; in <i>Technische Universit&#x000E4;t M&#x000FC;nchen, Munich, Germany, Tech. Rep. TUM-I1214</i>.</p>
<p class="ReferencesCopy2"><a href="http://scholar.google.com/scholar_lookup?title=&#x0201C;Fusing+multiple+kinects+to+survey+shared+human-robot-workspaces,&#x0201D;&#x00026;author=C.+Lenz&#x00026;author=M.+Grimm&#x00026;author=T.+R&#x000F6;der&#x00026;author=A.+Knoll&#x00026;journal=Technische+Universit&#x000E4;t+M&#x000FC;nchen,+Munich,+Germany,+Tech.+Rep.+TUM-I1214&#x00026;publication_year=2012" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B16"></a>Lew, J. Y., Jou, Y.-T., and Pasic, H. (2000). &#x0201C;Interactive control of human/robot sharing same workspace,&#x0201D; in <i>Proc. IEEE/RSJ Int. Conf. Intelligent Robots and Systems</i>, Vol. 1 (Takamatsu: IEEE), 535&#x02013;540.</p>
<p class="ReferencesCopy2"><a href="http://scholar.google.com/scholar_lookup?title=&#x0201C;Interactive+control+of+human/robot+sharing+same+workspace,&#x0201D;&#x00026;author=J.+Y.+Lew&#x00026;author=Y.+T.+Jou&#x00026;author=H.+Pasic&#x00026;publication_year=2000&#x00026;volume=1" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B17"></a>Lewis, J. R. (2002). Psychometric evaluation of the PSSUQ using data from five years of usability studies. <i>Int. J. Hum. Comput. Interact.</i> 14, 463&#x02013;488. doi:10.1080/10447318.2002.9669130</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1080/10447318.2002.9669130" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?title=Psychometric+evaluation+of+the+PSSUQ+using+data+from+five+years+of+usability+studies&#x00026;author=J.+R.+Lewis&#x00026;journal=Int.+J.+Hum.+Comput.+Interact.&#x00026;publication_year=2002&#x00026;volume=14&#x00026;pages=463&#x02013;488&#x00026;doi=10.1080/10447318.2002.9669130" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B18"></a>Mar&#x000ED;n, R., Sanz, P. J., Nebot, P., and Wirz, R. (2005). A multimodal interface to control a robot arm via the web: a case study on remote programming. <i>IEEE Trans. Indus. Electron.</i> 52, 1506&#x02013;1520. doi:10.1109/TIE.2005.858733</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1109/TIE.2005.858733" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?title=A+multimodal+interface+to+control+a+robot+arm+via+the+web:+a+case+study+on+remote+programming&#x00026;author=R.+Mar&#x000ED;n&#x00026;author=P.+J.+Sanz&#x00026;author=P.+Nebot&#x00026;author=R.+Wirz&#x00026;journal=IEEE+Trans.+Indus.+Electron.&#x00026;publication_year=2005&#x00026;volume=52&#x00026;pages=1506&#x02013;1520&#x00026;doi=10.1109/TIE.2005.858733" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B19"></a>McGhan, C. L., Nasir, A., and Atkins, E. M. (2015). Human intent prediction using Markov decision processes. <i>J. Aerosp. Inf. Syst.</i> 12, 393&#x02013;397. doi:10.2514/1.I010090</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.2514/1.I010090" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?title=Human+intent+prediction+using+Markov+decision+processes&#x00026;author=C.+L.+McGhan&#x00026;author=A.+Nasir&#x00026;author=E.+M.+Atkins&#x00026;journal=J.+Aerosp.+Inf.+Syst.&#x00026;publication_year=2015&#x00026;volume=12&#x00026;pages=393&#x02013;397&#x00026;doi=10.2514/1.I010090" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B20"></a>Menchaca-Brandan, M. A., Liu, A. M., Oman, C. M., and Natapoff, A. (2007). &#x0201C;Influence of perspective-taking and mental rotation abilities in space teleoperation,&#x0201D; in <i>Proc. ACM/IEEE Int. Conf. Human-Robot Interaction</i> (Washington, DC: IEEE), 271&#x02013;278.</p>
<p class="ReferencesCopy2"><a href="http://scholar.google.com/scholar_lookup?title=&#x0201C;Influence+of+perspective-taking+and+mental+rotation+abilities+in+space+teleoperation,&#x0201D;&#x00026;author=M.+A.+Menchaca-Brandan&#x00026;author=A.+M.+Liu&#x00026;author=C.+M.+Oman&#x00026;author=A.+Natapoff&#x00026;publication_year=2007" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B21"></a>Micire, M., Desai, M., Courtemanche, A., Tsui, K., and Yanco, H. (2009). &#x0201C;Analysis of natural gestures for controlling robot teams on multi-touch tabletop surfaces,&#x0201D; in <i>Proc. ACM Int. Conf. Interactive Tabletops and Surfaces</i> (Banff: ACM), 41&#x02013;48.</p>
<p class="ReferencesCopy2"><a href="http://scholar.google.com/scholar_lookup?title=&#x0201C;Analysis+of+natural+gestures+for+controlling+robot+teams+on+multi-touch+tabletop+surfaces,&#x0201D;&#x00026;author=M.+Micire&#x00026;author=M.+Desai&#x00026;author=A.+Courtemanche&#x00026;author=K.+Tsui&#x00026;author=H.+Yanco&#x00026;publication_year=2009" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B22"></a>Milgram, P., Zhai, S., and Drascic, D. (1993). &#x0201C;Applications of augmented reality for human-robot communication,&#x0201D; in <i>Proc. IEEE/RSJ Int. Conf. Intelligent Robots and Systems</i>, Vol. 3 (Yokohama: IEEE), 1467&#x02013;1472.</p>
<p class="ReferencesCopy2"><a href="http://scholar.google.com/scholar_lookup?title=&#x0201C;Applications+of+augmented+reality+for+human-robot+communication,&#x0201D;&#x00026;author=P.+Milgram&#x00026;author=S.+Zhai&#x00026;author=D.+Drascic&#x00026;publication_year=1993&#x00026;volume=3" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B23"></a>Morato, C., Kaipa, K. N., Zhao, B., and Gupta, S. K. (2014). Toward safe human robot collaboration by using multiple kinects based real-time human tracking. <i>J. Comput. Inf. Sci. Eng.</i> 14, 011006. doi:10.1115/1.4025810</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1115/1.4025810" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?title=Toward+safe+human+robot+collaboration+by+using+multiple+kinects+based+real-time+human+tracking&#x00026;author=C.+Morato&#x00026;author=K.+N.+Kaipa&#x00026;author=B.+Zhao&#x00026;author=S.+K.+Gupta&#x00026;journal=J.+Comput.+Inf.+Sci.+Eng.&#x00026;publication_year=2014&#x00026;volume=14&#x00026;pages=011006&#x00026;doi=10.1115/1.4025810" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B24"></a>Muszynski, S., St&#x000FC;ckler, J., and Behnke, S. (2012). &#x0201C;Adjustable autonomy for mobile teleoperation of personal service robots,&#x0201D; in <i>Proc. IEEE Int. Symp. Robot and Human Interactive Communication</i> (Paris: IEEE), 933&#x02013;940.</p>
<p class="ReferencesCopy2"><a href="http://scholar.google.com/scholar_lookup?title=&#x0201C;Adjustable+autonomy+for+mobile+teleoperation+of+personal+service+robots,&#x0201D;&#x00026;author=S.+Muszynski&#x00026;author=J.+St&#x000FC;ckler&#x00026;author=S.+Behnke&#x00026;publication_year=2012" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B25"></a>Nielsen, C. W., Goodrich, M. A., and Ricks, R. W. (2007). Ecological interfaces for improving mobile robot teleoperation. <i>IEEE Trans. Robot.</i> 23, 927&#x02013;941. doi:10.1109/TRO.2007.907479</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1109/TRO.2007.907479" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?title=Ecological+interfaces+for+improving+mobile+robot+teleoperation&#x00026;author=C.+W.+Nielsen&#x00026;author=M.+A.+Goodrich&#x00026;author=R.+W.+Ricks&#x00026;journal=IEEE+Trans.+Robot.&#x00026;publication_year=2007&#x00026;volume=23&#x00026;pages=927&#x02013;941&#x00026;doi=10.1109/TRO.2007.907479" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B26"></a>Oberkampf, D., DeMenthon, D. F., and Davis, L. S. (1996). Iterative pose estimation using coplanar feature points. <i>Comput. Vision Image Understand.</i> 63, 495&#x02013;511. doi:10.1006/cviu.1996.0037</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1006/cviu.1996.0037" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?title=Iterative+pose+estimation+using+coplanar+feature+points&#x00026;author=D.+Oberkampf&#x00026;author=D.+F.+DeMenthon&#x00026;author=L.+S.+Davis&#x00026;journal=Comput.+Vision+Image+Understand.&#x00026;publication_year=1996&#x00026;volume=63&#x00026;pages=495&#x02013;511&#x00026;doi=10.1006/cviu.1996.0037" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B27"></a>Sakamoto, D., Sugiura, Y., Inami, M., and Igarashi, T. (2016). Graphical instruction for home robots. <i>Computer</i> 49, 20&#x02013;25. doi:10.1109/MC.2016.195</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1109/MC.2016.195" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?title=Graphical+instruction+for+home+robots&#x00026;author=D.+Sakamoto&#x00026;author=Y.+Sugiura&#x00026;author=M.+Inami&#x00026;author=T.+Igarashi&#x00026;journal=Computer&#x00026;publication_year=2016&#x00026;volume=49&#x00026;pages=20&#x02013;25&#x00026;doi=10.1109/MC.2016.195" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B28"></a>Sauro, J., and Lewis, J. R. (2012). <i>Quantifying the User Experience: Practical Statistics for User Research</i>. San Francisco, CA: Morgan Kaufmann.</p>
<p class="ReferencesCopy2"><a href="http://scholar.google.com/scholar_lookup?title=Quantifying+the+User+Experience:+Practical+Statistics+for+User+Research&#x00026;author=J.+Sauro&#x00026;author=J.+R.+Lewis&#x00026;publication_year=2012" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B29"></a>Shah, J., and Breazeal, C. (2010). An empirical analysis of team coordination behaviors and action planning with application to human&#x02013;robot teaming. <i>Hum. Factors</i> 52, 234&#x02013;245. doi:10.1177/0018720809350882</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1177/0018720809350882" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?title=An+empirical+analysis+of+team+coordination+behaviors+and+action+planning+with+application+to+human&#x02013;robot+teaming&#x00026;author=J.+Shah&#x00026;author=C.+Breazeal&#x00026;journal=Hum.+Factors&#x00026;publication_year=2010&#x00026;volume=52&#x00026;pages=234&#x02013;245&#x00026;doi=10.1177/0018720809350882" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B30"></a>Spong, M., Hutchinson, S., and Vidyasagar, M. (2006). <i>Robot Modeling and Control</i>. New York, NY: John Wiley and Sons.</p>
<p class="ReferencesCopy2"><a href="http://scholar.google.com/scholar_lookup?title=Robot+Modeling+and+Control&#x00026;author=M.+Spong&#x00026;author=S.+Hutchinson&#x00026;author=M.+Vidyasagar&#x00026;publication_year=2006" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B31"></a>Su, Y.-H., Hsiao, C.-C., and Young, K.-Y. (2015). &#x0201C;Manipulation system design for industrial robot manipulators based on tablet PC,&#x0201D; in <i>Intelligent Robotics and Applications</i> (Portsmouth, UK: Springer), 27&#x02013;36.</p>
<p class="ReferencesCopy2"><a href="http://scholar.google.com/scholar_lookup?title=&#x0201C;Manipulation+system+design+for+industrial+robot+manipulators+based+on+tablet+PC,&#x0201D;&#x00026;author=Y.+H.+Su&#x00026;author=C.+C.+Hsiao&#x00026;author=K.+Y.+Young&#x00026;publication_year=2015" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B32"></a>Waldherr, S., Romero, R., and Thrun, S. (2000). A gesture based interface for human-robot interaction. <i>Auton. Robots</i> 9, 151&#x02013;173. doi:10.1023/A:1008918401478</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1023/A:1008918401478" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?title=A+gesture+based+interface+for+human-robot+interaction&#x00026;author=S.+Waldherr&#x00026;author=R.+Romero&#x00026;author=S.+Thrun&#x00026;journal=Auton.+Robots&#x00026;publication_year=2000&#x00026;volume=9&#x00026;pages=151&#x02013;173&#x00026;doi=10.1023/A:1008918401478" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B33"></a>Woods, D. D., Tittle, J., Feil, M., and Roesler, A. (2004). Envisioning human-robot coordination in future operations. <i>IEEE Trans. Syst. Man. Cybern. C Appl. Rev.</i> 34, 210&#x02013;218. doi:10.1109/TSMCC.2004.826272</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1109/TSMCC.2004.826272" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?title=Envisioning+human-robot+coordination+in+future+operations&#x00026;author=D.+D.+Woods&#x00026;author=J.+Tittle&#x00026;author=M.+Feil&#x00026;author=A.+Roesler&#x00026;journal=IEEE+Trans.+Syst.+Man.+Cybern.+C+Appl.+Rev.&#x00026;publication_year=2004&#x00026;volume=34&#x00026;pages=210&#x02013;218&#x00026;doi=10.1109/TSMCC.2004.826272" target="_blank">Google Scholar</a></p></div>
<div class="References" style="margin-bottom:0.5em;">
<p class="ReferencesCopy1"><a id="B34"></a>Ziaei, Z., Hahto, A., Mattila, J., Siuko, M., and Semeraro, L. (2011). Real-time markerless augmented reality for remote handling system in bad viewing conditions. <i>Fusion Eng. Design</i> 86, 2033&#x02013;2038. doi:10.1016/j.fusengdes.2010.12.082</p>
<p class="ReferencesCopy2"><a href="https://doi.org/10.1016/j.fusengdes.2010.12.082" target="_blank">CrossRef Full Text</a> | <a href="http://scholar.google.com/scholar_lookup?title=Real-time+markerless+augmented+reality+for+remote+handling+system+in+bad+viewing+conditions&#x00026;author=Z.+Ziaei&#x00026;author=A.+Hahto&#x00026;author=J.+Mattila&#x00026;author=M.+Siuko&#x00026;author=L.+Semeraro&#x00026;journal=Fusion+Eng.+Design&#x00026;publication_year=2011&#x00026;volume=86&#x00026;pages=2033&#x02013;2038&#x00026;doi=10.1016/j.fusengdes.2010.12.082" target="_blank">Google Scholar</a></p></div>
</div>
<div class="thinLineM20"></div>
<div class="AbstractSummary">
<p><span>Keywords:</span> interaction, interface, robotics, manipulation, mixed-reality, tablet, vision, workspace</p>
<p><span>Citation:</span> Frank JA, Moorhead M and Kapila V (2017) Mobile Mixed-Reality Interfaces That Enhance Human&#x02013;Robot Interaction in Shared Spaces. <i>Front. Robot. AI</i> 4:20. doi: 10.3389/frobt.2017.00020</p>
<p id="timestamps"><span>Received:</span> 21 October 2016; <span>Accepted:</span> 10 May 2017;<br/> <span>Published:</span> 09 June 2017</p>
<div>
<p>Edited by:</p>
<a href="http://www.frontiersin.org/people/u/279005">Trung Dung Ngo</a>, University of Prince Edward Island, Canada</div>
<div>
<p>Reviewed by:</p>
<a href="http://www.frontiersin.org/people/u/59742">Fady Alnajjar</a>, RIKEN Brain Science Institute (BSI), Japan<br/> <a href="http://www.frontiersin.org/people/u/427082">Yongsheng Ou</a>, Shenzhen Institutes of Advanced Technology (CAS), China</div>
<p><span>Copyright:</span> &#x000A9; 2017 Frank, Moorhead and Kapila. This is an open-access article distributed under the terms of the <a rel="license" href="http://creativecommons.org/licenses/by/4.0/" target="_blank">Creative Commons Attribution License (CC BY)</a>. The use, distribution or reproduction in other forums is permitted, provided the original author(s) or licensor are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</p>
<p><span>*Correspondence:</span> Vikram Kapila, <a href="mailto:vkapila&#x00040;nyu.edu">vkapila&#x00040;nyu.edu</a></p>
<div class="clear"></div>
</div>                
                <div class="thin-line-dark"></div>
                    <div class="social-feed"></div>
                    <div class="container-bordered comment-list social-feed">
                        <div class="container-comments">                           
                        </div>

                        <div class="loading-wrapper" style="padding-left: 49%">
                            <img style="padding-top: 5%; padding-bottom: 5%;" src="/areas/research-topics/images/frontiers/common/icon/loading.gif" alt="Loading.." />
                        </div>                       
                    </div>

            </div>
        </div>
    </div>
</main>

            <div class="clearfix visible-sm"></div>

            
    <div class="c col-xs-12 col-sm-12 col-lg-2 col-md-3 pull-right side-article related right-container-articles" style="clear: right !important">
        
        <article class="widget-listing commentary-article hidden">
            <h5 class="like-h4">COMMENTARY</h5>
        </article>

        <article class="widget-listing original-article hidden">
            <h5 class="like-h4">ORIGINAL ARTICLE</h5>
        </article>

        <article class="widget-listing people-also-looked-at side-article-related hidden">
            <h5 class="like-h4">People also looked at</h5>
        </article>
            <div class="side-article-mrk clearfix hidden-md hidden-lg">
                <style type="text/css"><!--
.spotlight-title:hover,
.spotlight-title:focus {
 color: #555 !important; 
}
.spotlight-link {
 color: #f2b12f !important;
}
.spotlight-link:hover,
.spotlight-link:focus {
 color: #e5a812 !important;
}
--></style>
<div class="spotlight"><a href="https://spotlight.frontiersin.org/how-to-enter/?utm_source=FWEB&amp;utm_medium=FMAIN&amp;utm_campaign=SPOTLIGHT&amp;utm_term=BANNER&amp;utm_content=WEB"><img src="https://3718aeafc638f96f5bd6-d4a9ca15fc46ba40e71f94dec0aad28c.ssl.cf1.rackcdn.com/article-marketing-message-1f3be380-af93-4b04-a218-32e22eb3bdda.png" alt="" /></a><br />
<h3 style="font-weight: 500;"><a class="spotlight-title" tabindex="0" href="https://spotlight.frontiersin.org/how-to-enter/?utm_source=FWEB&amp;utm_medium=FMAIN&amp;utm_campaign=SPOTLIGHT&amp;utm_term=BANNER&amp;utm_content=WEB" target="_blank">Want to win $100,000 to host your own conference?</a></h3>
<a class="spotlight-link" href="https://spotlight.frontiersin.org/how-to-enter/?utm_source=FWEB&amp;utm_medium=FMAIN&amp;utm_campaign=SPOTLIGHT&amp;utm_term=BANNER&amp;utm_content=WEB" target="_blank">Suggest a Research Topic</a></div>
            </div>


    </div>


        </div>
    </div>
</div>
<div id="divTemplates">
    <div class="modal fade modal-container impact-modal-container" data-backdrop="static" id="impactModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
</div>






<script type="text/template" id="template-impact-modal">

    <div class="container">
        <div class="row">
            <div style="padding-bottom: 100px">
                <div class="modal-dialog">
                    <div class="modal-content">
                        <div class="modal-header clearfix">
                            <button type="button" class="close btn-close" data-dismiss="modal" aria-hidden="true">&times;</button>
                            <h3 class="modal-title" id="myModalLabel">Impact</h3>
                            <h6>
                                <a href="javascript:void(0)" class="article-title" data-dismiss="modal" aria-hidden="true">
                                    Mobile Mixed-Reality Interfaces That Enhance Human–Robot Interaction in Shared Spaces
                                </a>
                            </h6>
                        </div>
                        <div class="modal-body">
                            <img class="loader" src="/areas/journals/images/icon/loading.gif" alt="Loading.." />

                            <div class="row article-impact-tabs hidden">
                                <!-- Nav tabs -->
                                <ul class="nav nav-tabs impact-tabs clearfix">
                                    <li class="active nav-tab-views"><a href="#views-tab" data-toggle="tab" data-tab-type="views"><span class="total-views" data-tab-type="views"></span>Views</a></li>
                                    <li class="nav-tab-citations"><a href="#citations-tab" data-toggle="tab" data-tab-type="citations"><span class="highest-Citation" data-tab-type="citations"></span>Citations</a></li>
                                    <li class="nav-tab-demographics"><a href="#demographics-tab" data-toggle="tab" data-tab-type="demographics">Demographics</a></li>
                                        <li class="nav-tab-social-buzz"><a href="#social-buzz-tab" data-toggle="tab" data-tab-type="socialbuzz">Social Buzz</a></li>
                                </ul>
                                <div class="tab-content impact-tabs">
                                    <!-- VIEWS TAB -->
                                    <div class="tab-pane active" id="views-tab">
                                        <h5 style="padding-left: 68px; display: none;" class="sincebeginning">Since beginning</h5>
                                        <h5 style="padding-left: 68px; display: none;" class="thisyear">Last 12 months</h5>
                                        <h5 style="padding-left: 68px; display: none;" class="thismonth">Last 30 days</h5>


                                        <p style="display: none;height:60px" class="views-empty-message lead">No records found</p>
                                        <div class="chart-container">
                                            <p class="info sincebeginning" style="display: none;"><strong class="begin-views-downloads download-font-styles"> </strong> views and downloads <strong class="begin-views year-font-styles"> </strong> views <strong class="begin-downloads year-font-styles"> </strong> downloads</p>
                                            <p class="info thisyear" style="display: none;"><strong class="year-views-downloads download-font-styles"> </strong> views and downloads <strong class="year-views year-font-styles"> </strong> views <strong class="year-downloads year-font-styles"> </strong> downloads</p>
                                            <p class="info thismonth" style="display: none;"><strong class="month-views-downloads download-font-styles"> </strong> views and downloads <strong class="month-views year-font-styles"> </strong> views <strong class="month-downloads year-font-styles"> </strong> downloads</p>
                                            <div id="views-timeline-chart" class="views-bar-charts"></div>
                                            <div id="views-bar-chart" class="views-timeline-charts"></div>
                                        </div>
                                        <div id="views-chart-controls" class="input-block-level" style="display: none;">
                                            <!-- Split button -->
                                            <div class="btn-group pull-right">
                                                <button type="button" class="btn btn-flat dropdown-toggle" data-toggle="dropdown" >
                                                    <span id="Period" data-bind="label">Select a time period</span>&nbsp;<span class="caret"></span>
                                                </button>
                                                <ul id="PeriodDrop" class="dropdown-menu" role="menu">
                                                    <li class="beginning" ><a>Since beginning</a></li>
                                                    <li class="year"><a>Last 12 months</a></li>
                                                    <li class="month"><a>Last 30 days</a></li>
                                                </ul>
                                            </div>
                                            <div style="padding-left: 72px;" class="btn-group pull-left" data-toggle="tooltip" title="Toggle between timeline and bar chart views">
                                                <button type="button" class="btn btn-flat btn-line active" id="views-timeline-button">
                                                    <span style="padding: 0 4px">&nbsp;&nbsp;</span>
                                                </button>
                                                <button type="button" class="btn btn-flat btn-line" id="views-bar-button">
                                                    <span style="padding: 0 4px">&nbsp;&nbsp;</span>
                                                </button>

                                            </div>
                                            <div class="clearfix"></div>
                                        </div><!-- /#views-chart-controls -->

                                    </div><!-- /#views-tab -->
                                    <!-- CITATIONS TAB -->
                                    <div class="tab-pane " id="citations-tab">
                                        <img class="citation-loader" src="/Images/Frontiers/Common/Icon/loading.gif" alt="Loading.." />
                                        <div style="width: 350px; margin: 0 auto;" class="citation-container hidden">
                                            <h5 style="text-align:center; margin-bottom: 0; padding-bottom: 0;">Article Citations</h5>
                                            <div class="scopus" style="width: 50%; float: left; text-align: center;">
                                                <a href= "http://www.scopus.com/inward/citedby.url?partnerID=HzOxMe3b&amp;doi=10.3389/frobt.2017.00020&amp;origin=inward"><h5><span class="repository">by Scopus</span></h5></a>
                                                <!-- Split button -->
                                                <div class="citation-count-container">
                                                    <div class="citation-panel">
                                                        <p class="citation-count"><span class="citaion-scopus"></span></p>
                                                    </div><!-- /.citation-panel -->
                                                </div><!-- /.well -->
                                            </div>

                                            <div class="crossref" style="width: 50%; float: right; text-align: center;">
                                                <a href="http://www.crossref.org/"><h5><span class="repository">by CrossRef</span></h5></a>
                                                <!-- Split button -->
                                                <div class="clearfix"></div>
                                                <div class="citation-count-container">
                                                    <div class="citation-panel">
                                                        <p class="citation-count"><span class="citation-crossref"></span></p>
                                                    </div><!-- /.citation-panel -->

                                                </div><!-- /.well -->
                                            </div>
                                            <div class="clearfix"></div>
                                        </div>
                                    </div><!-- /#citations-tab -->
                                    <!-- DEMOGRAPHICS TAB -->
                                    <div class="tab-pane " id="demographics-tab">

                                        <div class="btn-group pull-right" id="demographics-dropdown" style="margin-right: 10px;">
                                            &nbsp;
                                        </div>

                                        <div id="map-container">

                                            <div id="map">

                                            </div>
                                            <img class="map-loading" src="/Images/Frontiers/Common/Icon/loading.gif" alt="Loading.." />
                                        </div>
                                        <div class="clearfix"></div>

                                        <div class="row">
                                            <div class="col-xs-6">
                                                <h5>Top countries</h5>
                                                <div id="demographics-top-countries"></div>
                                            </div>
                                            <div class="col-xs-6">
                                                <h5>Top referring sites</h5>
                                                <div id="demographics-top-sites"></div>
                                            </div>
                                        </div>
                                        <hr>
                                        <div class="row">
                                            <div class="col-xs-4">
                                                <h5 style="text-align: center;">Domain</h5>
                                                <div id="demographics-domain"></div>
                                            </div>
                                            <div class="col-xs-4">
                                                <h5 style="text-align: center;">Field</h5>
                                                <div id="demographics-field"></div>
                                            </div>
                                            <div class="col-xs-4">
                                                <h5 style="text-align: center;">Specialty</h5>
                                                <div id="demographics-specialty"></div>
                                            </div>
                                        </div>
                                        <hr>
                                        <div class="row">
                                            <div class="col-xs-4">
                                                <h5 style="text-align: center;">Industry</h5>
                                                <div id="demographics-industry"></div>
                                            </div>

                                            <div class="col-xs-4">
                                                <h5 style="text-align: center;">Education</h5>
                                                <div id="demographics-education"></div>
                                            </div>

                                            <div class="col-xs-4">
                                                <h5 style="text-align: center;">Position</h5>
                                                <div id="demographics-position"></div>
                                            </div>
                                          
                                        </div>
                                        <div class="clearfix"></div>

                                        <div class="demographics-age-gender">
                                            <hr>
                                            <h5 style="margin: 35px 0px;">Age and Gender</h5>
                                            <div id="demographics-age" style="min-width: 800px; height: 400px; margin: 0 auto"></div>
                                        </div>
                                    </div><!-- /#demographics-tab -->
                                        <!-- SOCIAL BUZZ TAB -->
                                        <div class="tab-pane " id="social-buzz-tab">
                                            <h5 class="altmetric-title">Altmetric</h5>
                                            <div id="altmetric-donut" data-badge-details="right" data-badge-type="medium-donut" data-doi="<%- doi %>" data-hide-no-mentions="false" class="altmetric-embed"></div>

                                            <div id='main' class="social-buzz-content-area hidden">
                                                <div id='content'>
                                                    <div id='details'>
                                                        <div class="row">
                                                            <div class="col-xs-6">
                                                                <h5>Sources</h5>
                                                            </div>
                                                            <div class="col-xs-6">
                                                                <ul class="nav nav-tabs pull-right">
                                                                    <li class='active'><a href="#altmetric-all-tab"  data-toggle="tab">All</a></li>
                                                                    <li id='tab_for_msm'><a href="#details_msm" data-toggle="tab">News</a></li>
                                                                    <li id='tab_for_blogs'><a href="#details_blogs" data-toggle="tab">Blogs</a></li>
                                                                </ul>
                                                            </div>
                                                        </div>

                                                        <div class="tabbable">
                                                            <div class="tab-content">

                                                                <div class='details tab-pane ' id='details_blogs'>

                                                                    <div class="blogs-posts"></div>
                                                                </div> <!--Blog-->

                                                                <div class='details tab-pane' id='details_msm'>

                                                                    <div class="news-posts"></div>
                                                                </div> <!--News-->

                                                                <div class='details tab-pane active' id='altmetric-all-tab'>
                                                                    <div class="all-posts"></div>
                                                                </div> <!--All-->

                                                            </div>
                                                        </div>

                                                    </div>

                                                    <div class='clearfix'></div>
                                                </div>
                                            </div>

                                        </div>
                                        <!-- /#social-buzz-tab -->
                                </div>
                            </div><!-- /.row -->

                        </div><!-- /.modal-body -->
                        <div class="modal-footer">
                            <span class="modal-footer-pmc">The displayed data aggregates results from <a href="https://www.frontiersin.org">Frontiers</a> and <a href="http://www.ncbi.nlm.nih.gov/pmc/">PubMed Central®</a>.</span>
                            <button type="button" class="btn btn-new-orange btn-flat btn-close" data-dismiss="modal">Close</button>
                        </div>
                    </div><!-- /.modal-content -->
                </div><!-- /.modal-dialog -->
            </div><!-- /.col-xs-9 -->
        </div><!-- /.row -->
    </div><!-- /.container -->

</script>



<script type="text/template" id="template-social-buzz-blogs-posts">
    <div class='posts_info'>So far Altmetric has seen <b><%- blogsCount %></b> posts.</div>
    <% _.each( socialBuzzPosts, function( socialBuzzPost ){ %>

    <div class='postbox'>
        <div class='title'><a target='_blank' href="<%- socialBuzzPost.Url%>" ><%- socialBuzzPost.Title %></a></div>
        <div class='source'><a target='_blank' href="<%- socialBuzzPost.Author.Url %>" ><%- socialBuzzPost.Author.Name %></a></div>
        <div class='snippet'>
            <p><%- socialBuzzPost.Summary %></p>
        </div>
        <div class="footer blog">
            <%-  FRArticleImpact.formattedPostedDate(socialBuzzPost.PostedOn) %>
        </div>
    </div>

    <% }); %>

</script>


<script type="text/template" id="template-social-buzz-news-posts">
    <div class='posts_info'>So far Altmetric has seen <b><%- newsCount %></b> stories.</div>
    <% _.each( socialBuzzPosts, function( socialBuzzPost ){ %>
    <div class='postbox'>
        <div class='author source' style='width: 80px;'>
            <div style='max-width: 80px; overflow: hidden;'>
                <img width='80' src="<%- socialBuzzPost.Author.Image %>" class="photo" />
            </div>
        </div>

        <div class="entry-content" style="margin-left: 0px;">
            <div class='title'>
                <a target='_blank' href="<%- socialBuzzPost.Url%>"><%- socialBuzzPost.Title %> </a>
            </div>
            <div class='source'><%- socialBuzzPost.Author.Name %></div>
            <div class='snippet'>
                <p><%- socialBuzzPost.Summary %></p>
            </div>
        </div>
        <div class="footer">
            <%-  FRArticleImpact.formattedPostedDate(socialBuzzPost.PostedOn) %>
        </div>
    </div>

    <% }); %>

</script>


<script type="text/template" id="template-social-buzz-all-posts">

    <div class="posts_info">So far Altmetric has seen <b><%- allCount %></b> stories.</div>

    <% _.each( socialBuzzPosts, function( socialBuzzPost ){ %>
    <% if (socialBuzzPost.Type == "Blogs") {%>

    <div class='postbox'>
        <div class='title'><a target='_blank' href="<%- socialBuzzPost.Url%>" ><%- socialBuzzPost.Title %></a></div>
        <div class='source'><a target='_blank' href="<%- socialBuzzPost.Author.Url %>" ><%- socialBuzzPost.Author.Name %></a></div>
        <div class='snippet'>
            <p><%- socialBuzzPost.Summary %></p>
        </div>
        <div class="footer blog">
            <%-  FRArticleImpact.formattedPostedDate(socialBuzzPost.PostedOn) %>
        </div>
    </div>

    <% } %>
    <%  if(socialBuzzPost.Type == "News"){ %>
    <div class='postbox'>
        <div class='author source' style='width: 80px;'>
            <div style='max-width: 80px; overflow: hidden;'>
                <img width='80' src="<%- socialBuzzPost.Author.Image %>" class="photo" />
            </div>
        </div>

        <div class="entry-content" style="margin-left: 0px;">
            <div class='title'>
                <a target='_blank' href="<%- socialBuzzPost.Url%>"><%- socialBuzzPost.Title %> </a>
            </div>
            <div class='source'><%- socialBuzzPost.Author.Name %></div>
            <div class='snippet'>
                <p><%- socialBuzzPost.Summary %></p>
            </div>
        </div>
        <div class="footer">
            <%-  FRArticleImpact.formattedPostedDate(socialBuzzPost.PostedOn)%>
        </div>
    </div>
    <% } %>
    <% }); %>

</script>






    <div class="modal fade modal-container supplementary-modal-container"
     data-backdrop="static"
     data-keyboard="false"
     id="supplementaryFilesModal"
     tabindex="-1" role="dialog"
     aria-labelledby="mySupplementaryModalLabel"
     aria-hidden="true">
</div>




<script type="text/template" id="template-supplementary-files-modal">
    <div class="supplementary-material-wrapper modal-dialog" role="document">
        <div class="modal-content">
            <div class="modal-body">
                <a class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></a>
                <h4>Supplementary Material</h4><br>
                <p class="supplementary-empty-message">There is no supplementary material currently available for this article</p>
                <div class="loading-wrapper"
                     id="loader"
                     style="display:none">
                    Loading supplemental data...
                    <img style="" src="/Areas/Articles/Images/Icon/loading.gif" alt="Loading.." />
                </div>
                <br />
                <div class="table-responsive" id="localFiles">
                </div>
                <div id="figshare-widget-container">

                </div>
            </div>
            <br />
            <div class="modal-footer bottom-links">
                <a class="btn btn-default" data-dismiss="modal">Close</a>
            </div>
        </div>

    </div>
</script>





<script type="text/template" id="template-local-files-modal">
    <table class="table table-striped supplementary-content" id="localFilesTable">
        <thead>
            <tr>
                <th>&nbsp;</th>
                <th>File Name</th>
                <th>&nbsp;</th>
            </tr>
        </thead>
        <tbody class="file-content">
            <% _.each(supplimentalFileDetails.FileDetails, function(fileViewModel){   %>
            <tr>
                <th scope="row"><a data-test-id="<%= fileViewModel.TestId %>" href="<%= fileViewModel.FileDownloadUrl %>"><img class="img-figure" src="<%= fileViewModel.ImageUrl %>"></a></th>
                <td>
                    <a  href="<%= fileViewModel.FileDownloadUrl %>">
                        <%= fileViewModel.FileName %>
                    </a>
                </td>
                <td><a  href="<%= fileViewModel.FileDownloadUrl %>" class="btn-link"><i class="fa fa-download" aria-hidden="true"></i></a></td>
            </tr>
            <% }); %>
        </tbody>
    </table>
</script>







    <div class="modal fade modal-container notifyme-modal-container" data-backdrop="static" id="notifyModal" tabindex="-1" role="dialog" aria-labelledby="Notify on publication" aria-hidden="true">
</div>



<script type="text/template" id="template-notify-me-modal">
    <div class="modal-f page-container simple-modal" role="dialog" aria-labelledby="NotfifyMeModalLabel">
            <div class="modal-dialog" role="document">
                <div class="modal-content">
                    <form onsubmit="return false" id="modalnotifyme">
                        <div class="modal-header">
                            <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
                            <h3>Notify me on publication</h3>
                        </div>
                        <div class="modal-body">

                            <div class="form-group">
                                <label>Please enter your email address:</label>
                            <input type="email" required class="form-control" id="txt_notification_email_id" placeholder="Email">
                            </div>

                        <div class="g-recaptcha" id="recaptcha"></div><br />

                        <p class="small" style="color: #555;">If you already have an account, please <a href="<%=login.FrontiersLoginUrl%>?returnUrl=<%=currentPage%>" class="text-blue">login</a>.</p>
                        <p class="small" style="color: #555;">You don't have a Frontiers account ? You can <a href="<%=login.FrontiersRegistrationUrl%>" class="text-blue">register here</a>.</p>
                        </div>
                        <div class="modal-footer">
                        <button type="button" id="article_notify_non_registered_user" class="btn btn-default btn-progress" style="width: 118px; visibility:hidden;">Notify me</button>
                        </div>
                    </form>
                </div>
            </div>
        </div>
</script>

<script type="text/template" id="template-notifyme-mailselection-modal">
    <div class="modal-f page-container simple-modal" tabindex="-1" role="dialog" aria-labelledby="NotfifyMeModalLabel">
        <div class="modal-dialog" role="document">
            <div class="modal-content">
                <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
                    <h3>Select one of your emails</h3>
                </div>
                <div class="modal-body">
                    <form>
                        <div class="form-group">
                            <label>You have multiple emails registered with Frontiers:</label>
                        </div>
                                           
                        <% _.each(userEmailList, function(user){   %>
                        <input type="radio" value="<%=user%>" name="notify_Emailcollection"> <%=user%><br>
                        <% }); %>

                    </form>
                </div>
                <div class="modal-footer">
                    <button type="button" id="article_notify_loggedin_user" class="btn btn-default btn-progress" style="width: 118px;">Notify me</button>
                </div>
            </div>

        </div>

    </div>
</script>

<script type="text/template" id="template_notifyme_error_modal">
    <div class="modal-f page-container simple-modal" tabindex="-1" role="dialog" aria-labelledby="NotfifyMeModalLabel">
        <div class="modal-dialog" role="document">
            <div class="modal-content">
                <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
                    <h3>Notify me on publication</h3>
                </div>
                <div class="modal-body">
                    <form>
                        <div class="form-group">
                            <p class="text-red"><strong><%=message%></strong></p>
                        </div>
                    </form>
                </div>
                <div class="modal-footer">
                    <button type="button" data-dismiss="modal" class="btn btn-default btn-progress" style="width: 118px;">Cancel</button>
                </div>
            </div>
        </div>
    </div>
</script>







    

<script type="text/template" id="template-article-comments">

    <div class="w100pc float_left CommentsHolder" id="div_article_comment_holder">
        <% if(journalComments.Count !=0) { %>
        <div id="divActivityBox" class="commentBox bubbleInfo " style="z-index: 300;">
            <div class="float_left w100pc divActivityBox">
                <p id="pAllUsers" class="pb0 float_left w581 pAllUsers">
                    <a id="aAllUsers" href="javascript:void(0);" class="fontnormal trigger">
                        <%- journalComments.Count %> Comments
                    </a> <%= journalComments.CommentedUsers %>
                </p>
            </div>
        </div>
        <% } %>

        <% _.each( journalComments.Comments, function( comment ){ %>

        <div class="commentBox">

            <% if (comment.IsDeleted) {%>

            <div class="wrapper pt4 pb4">
                <div class="imgHolder mr10"> <!--<a  class="fontnormal">--><img width="32" src="/Design/Images/default_profile_32.jpg" onerror="this.src='/Design/Images/default_profile_32.jpg';" alt=""><!--</a>--></div>
                <div class="content w555">
                    <!-- <p class="mtm3" style='word-wrap:break-word'><a  class="fontnormal">User</a> </p>-->
                    <p class="mtm3">User</p><ul class="toolbar mt4 disblk w100pc">
                        <li class="pl0">
                            <p class="date">Comment deleted on <%- GetFormattedCommentDeletedDate(comment.CommentedDate) %></p>
                        </li>
                        <li class="float_right pr0"></li>
                    </ul>
                </div>
            </div>

            <% } else {%>

            <div class="wrapper pt4 pb4">
                <div class="imgHolder mr10"> <a class="fontnormal" data-test-id="article_commenteduser_profileimage_link" href="<%- comment.CommentedUser.ProfileUrl %>"><img src="<%- comment.CommentedUser.ThumbnailUrl %>" onerror="this.src='/Images/Frontiers/Common/Profile/default_profile_32.jpg';" alt=""></a></div>
                <div class="content w555 comment-body">
                    <p class="mtm3" style="word-wrap:break-word">
                        <a class="fontnormal" data-test-id="article_commenteduser_profile_link" href="<%- comment.CommentedUser.ProfileUrl %>">
                            <%- comment.CommentedUser.FullName %>
                        </a>
                        <span class="comment-text"><%= comment.Comment %></span>
                    </p>
                    <ul class="toolbar mt4 disblk w100pc">
                        <li class="pl0"><p class="date"><%- GetFormattedCommentDate(comment.CommentedDate) %></p></li>
                        <% if (comment.IsEditVisible)
                        { %>
                        <li>|</li>
                        <li><a href="javascript:void(0);" data-test-id="article_comment_edit_link" class="fontnormal btn-edit-comment" id="btnEditcom" data-comment-id="<%- comment.CommentId %>">Edit</a></li>
                        <% } %>
                        <% if (comment.IsDeleteVisible)
                        { %>
                        <li>|</li>
                        <li><a href="javascript:void(0);" data-test-id="article_comment_delete_link" class="fontnormal btn-delete-comment" id="btnDeletecom" data-comment-id="<%- comment.CommentId %>">Delete</a></li>
                        <% }%>
                        <li class="float_right pr0"></li>
                    </ul>
                </div>
            </div>
            <% }%>
        </div>

        <%  }); %>
    </div>

    <div id="div_add_comment" class="commentBox">
        <div class="imgTn32"
             style="display: none;">
            <% if(journalComments.LoginUserId !=0) { %>
            <img src="<%- journalComments.LoggedInUserInfo.ThumbnailUrl %>" alt=""
                 onerror="this.src='/Images/Frontiers/Common/Profile/default_profile_32.jpg';">
            <% } else {%>
            <img src="/Design/Images/default_profile_32.jpg" alt=""
                 onerror="this.src='/Images/Frontiers/Common/Profile/default_profile_32.jpg';">
            <% }%>

        </div>
        <div class="wrapper pt4 pb4 wAuto">
            <textarea onfocus="showUserImageOnAdd(this)"
                      onblur="hideUserImage(this)"
                      id="txt_comment"
                      cols=""
                      rows="1"
                      style="overflow: hidden; word-wrap: break-word; resize: horizontal;"
                      data-test-id="article_comment_add_textarea" 
                      class="write setfocus autoext w535 frUIResizeTextArea comment-textarea is-ui animated-resize grey_90">Write a comment...</textarea>
        </div>
        <div class="commentAddWrap">
            <div>
                <a class="addBtnComment" data-test-id="article_comment_add_button"  onclick="javascript:AddComments(this);" href="javascript:void(0);" id="btnAddcom"><span>Add</span></a>
            </div>
        </div>
    </div>

</script>

<script type="text/template" id="template-article-edit-comments">


    <div id="edit-comment-form" class="wrapper pt4 pb4 wAuto edit-comments">
        <textarea class="txt-comment"
                  onfocus="showUserImageOnEdit(this)"
                  onblur="hideUserImage(this)"
                    data-test-id="article_comment_edit_textarea" 
                  rows="3"><%= commentText %> </textarea>
        <div>
            <a class="btn btn-add-comment addBtnComment" data-test-id="article_comment_edit_button" data-comment-id="<%- commentId %>">Add</a>
        </div>
    </div>

</script>



</div>

    </div>

    
<footer class="footer-new">
    <div class="container-fluid main-container-xxl">
        <div class="col-lg-2 col-lg-offset-2 col-md-3 col-sm-3">
            <ul class="list-unstyled">
                <li><a href="https://www.frontiersin.org/Journal/Frontiers.aspx">Home</a></li>
                <li><a href="https://www.frontiersin.org/about">About Frontiers</a></li>
                <li><a href="https://www.frontiersin.org/about/journalseries">Journals A-Z</a></li>
                <li><a href="https://www.frontiersin.org/about/Institutional_Membership">Institutional Membership</a></li>

            </ul>
        </div>
        <div class="col-lg-2 col-md-3 col-sm-3">
            <ul class="list-unstyled">
                <li><a href="https://www.frontiersin.org/about/contact">Contact</a></li>
                
                <li><a href="https://blog.frontiersin.org">News</a></li>
            </ul>
        </div>
        <div class="col-lg-2 col-md-3 col-sm-3">
            <ul class="list-unstyled">
                <li><a href="https://www.frontiersin.org/submissioninfo">Submit</a></li>
                <li><a target="_blank" href="https://frontiers.zendesk.com/hc/en-us">FAQs</a></li>
                <li><a href="https://www.frontiersin.org/TermsandConditions.aspx">Terms &amp; Conditions</a></li>
                <li><a href="https://www.frontiersin.org/legal/privacy-policy">Privacy Policy</a></li>
            </ul>
        </div>
        <div class="col-lg-2 col-md-3 col-sm-3">
            <ul class="list-unstyled">
                <li><a href="http://connect.frontiersin.org/subscriptions/subscribe">Newsletters</a></li>
                <li><a href="https://blog.frontiersin.org/2013/11/01/frontiers-social-media-and-rss/">RSS/Twitter</a></li>
                <li><a href="https://www.frontiersin.org/Team.aspx">Team</a></li>
                <li><a href="https://www.frontiersin.org/Careers">Careers</a></li>
            </ul>
        </div>
    </div>
</footer>


<div class="footer-copyright-new">
    <div class="container-fluid main-container-xxl">
        <div class="col-md-12 text-center"> © 2007 - 2019 Frontiers Media S.A. All Rights Reserved </div>
    </div>
</div>


    <script src="//code.jquery.com/jquery-2.1.3.min.js"></script>

    <script src="/areas/articles/scripts/frontiers/common/frontiers.banners-1.0.js"></script>


<script src="https://code.jquery.com/ui/1.10.2/jquery-ui.min.js"></script>
<script src="https://crossmark.crossref.org/javascripts/v1.5/crossmark.min.js"></script>
        <script type="text/javascript">

            var $ = jQuery.noConflict(); //==>>ToDo: Avoid this..!!
        </script>

    <script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.1/js/bootstrap.min.js"></script>

    <script src="/areas/articles/js/vendors?v=m3ItdAkHsKHA4JnU2Jk5BuO77bBpDeX0_4_qxHTdIcE1"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
 

    <script src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
 

    <script src="/areas/articles/js/frontiers?v=5v0iGBYObQPdGzZnFPxtIO2dA9KNYK-QopChGFPTMCE1"></script>


    <script type="text/javascript">

        var FRArticle = (function() {
            return {
                ArticleId: '238204',
                DOI:'10.3389/frobt.2017.00020',
                LoginUserId: '0',
                DomainId: '3',
                FieldId: '72',
                SpecialityId: '1560',
                IsPreview: FRSafe.boolean('False'),
                IsViewImpactFromLoop: FRSafe.boolean('False'),
                IsPublished: 'True',
                FigShareApiUrl: 'https://api.figshare.com/v2/collections/search',
                FigShareTimeOut: '3000'
            };
        })();

        var FRSocial = (function() {
            return {
                itemId: 14,
                itemTypeId: 1,
                entityId: '238204',
                ownerId: '386707',
                sanPath: 'http://www.frontiersin.org/files/',
                subItemId: '8',
                loginUserId: '0',
                ownerNWDBId: '28',
                pageType: 1,
                loopUrl:'https://loop.frontiersin.org'
            };
        })();
    </script>

    <script>
        var FRAjaxSettings = (function () {

            function urlLowercase() {
                $.ajaxSetup({
                    beforeSend: function (jqXHR, settings) {
                        settings.url = settings.url.toLowerCase();
                    }
                });
            }

            return { urlLowercase: urlLowercase };
        })();

        FRAjaxSettings.urlLowercase();
    </script>

    <script src="https://api-journal.frontiersin.org/areas/header/content/scripts/frontiers.header.v3.js"></script><script src="https://api-journal.frontiersin.org/header/taxonomy?v=1557067779879"></script><script src="https://api-journal.frontiersin.org/header/userprofile"></script>

    <script type="text/javascript">

        if (!FRArticle.IsPreview) {

            (function(i, s, o, g, r, a, m) {
                i['GoogleAnalyticsObject'] = r;
                i[r] = i[r] || function() {
                    (i[r].q = i[r].q || []).push(arguments);
                }, i[r].l = 1 * new Date();
                a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
                a.async = 1;
                a.src = g;
                m.parentNode.insertBefore(a, m);
            })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

            ga('create', 'UA-9164039-1', 'www.frontiersin.org');
         
        }
    </script>

    
    <script src="/areas/articles/js/app?v=GXcf1aBTidJEeJeoa80-V9smBPn2Iey_Dc1EcQNS7IM1"></script>

    <script src="https://widgets.figshare.com/static/figshare.js"></script>


</body>
</html>
